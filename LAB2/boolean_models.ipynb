{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0706832",
   "metadata": {},
   "source": [
    "# Task 1 – Boolean Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a998963",
   "metadata": {},
   "source": [
    "## Classic boolean model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d600bc2",
   "metadata": {},
   "source": [
    "Each term is binary (present or not).\n",
    "\n",
    "Retrieve only documents that exactly satisfy the Boolean expression.\n",
    "\n",
    "Operators: AND, OR, NOT.\n",
    "\n",
    "Example query: q = (query AND reformulation) OR (Language AND model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(query AND reformulation) OR (language AND model)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec43351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classic Boolean Model Results:\n",
      "Query: (query AND reformulation) OR (language AND model)\n",
      "Retrieved documents: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ---------- 1. Preprocessing ----------\n",
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stops]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------- 2. Load Inverted Index ----------\n",
    "def load_inverted_index(filepath):\n",
    "    inverted = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                term, doc_id = parts[0], parts[1]\n",
    "                inverted.setdefault(term, set()).add(doc_id)\n",
    "    return inverted\n",
    "\n",
    "\n",
    "# ---------- 3. Evaluate Boolean Query ----------\n",
    "def evaluate_boolean_query(query, inverted_index, all_docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # Extract all candidate words (ignoring AND/OR/NOT and parentheses)\n",
    "    raw_tokens = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    unique_tokens = set(raw_tokens) - {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "    # Start with the original expression\n",
    "    expression = query\n",
    "\n",
    "    # For each token, find its stemmed version and replace it\n",
    "    for token in unique_tokens:\n",
    "        if token.lower() in stops:\n",
    "            continue\n",
    "        stemmed = stemmer.stem(token.lower())\n",
    "        docs = inverted_index.get(stemmed, set())\n",
    "        expression = re.sub(rf'\\b{token}\\b', f\"set({list(docs)})\", expression, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace logical operators with Python equivalents\n",
    "    expression = re.sub(r\"\\bAND\\b\", \"&\", expression, flags=re.IGNORECASE)\n",
    "    expression = re.sub(r\"\\bOR\\b\", \"|\", expression, flags=re.IGNORECASE)\n",
    "    expression = re.sub(r\"\\bNOT\\b\", \"all_docs -\", expression, flags=re.IGNORECASE)\n",
    "\n",
    "    # Evaluate expression safely\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": None}, {\"all_docs\": all_docs, \"set\": set})\n",
    "    except Exception as e:\n",
    "        print(\"Error in query:\", e)\n",
    "        print(\"Expression after replacements:\", expression)\n",
    "        return set()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------- 4. Example Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    inverted_index = load_inverted_index(\"results/inverted_index_weighted.txt\")\n",
    "    all_docs = {f\"D{i}\" for i in range(1, 7)}\n",
    "\n",
    "    relevant_docs = evaluate_boolean_query(query, inverted_index, all_docs)\n",
    "\n",
    "    print(\"\\nClassic Boolean Model Results:\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Retrieved documents:\", sorted(relevant_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e7c1e",
   "metadata": {},
   "source": [
    "## Fuzzy boolean model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76864765",
   "metadata": {},
   "source": [
    "### 🔹 1. Concept Recap\n",
    "\n",
    "The **Fuzzy Boolean Model** is a hybrid between:\n",
    "\n",
    "- the **Boolean model** (logical operators `AND`, `OR`, `NOT`)  \n",
    "- and the **Vector model** (graded, real-valued similarities instead of strict true/false).\n",
    "\n",
    "Each term weight (e.g., **TF-IDF**) is treated as a **degree of membership** in the interval **[0, 1]**, not binary.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 Core idea\n",
    "\n",
    "For each query term \\( t \\) and document \\( d \\):\n",
    "\n",
    "\\[\n",
    "w_{t,d} = \\text{TF-IDF weight of term } t \\text{ in document } d\n",
    "\\]\n",
    "\n",
    "Each document’s relevance to a query is computed using **fuzzy logic operators**:\n",
    "\n",
    "- **AND →** use `min`\n",
    "- **OR →** use `max`\n",
    "- **NOT →** use `1 - weight`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Fuzzy Boolean Operators\n",
    "\n",
    "| Operator | Boolean | Fuzzy Equivalent | Formula |\n",
    "|:---------:|:--------:|:----------------:|:--------:|\n",
    "| **AND** | ∧ | min | \\( S_{AND}(d,q) = \\min(w_{t1,d}, w_{t2,d}) \\) |\n",
    "| **OR** | ∨ | max | \\( S_{OR}(d,q) = \\max(w_{t1,d}, w_{t2,d}) \\) |\n",
    "| **NOT** | ¬ | complement | \\( S_{NOT}(d,q) = 1 - w_{t,d} \\) |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Note:**  \n",
    "For multi-term queries, you can combine these operators **recursively** to compute the final fuzzy relevance score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "971fd92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Boolean results:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "4: 0\n",
      "5: 0\n",
      "6: 0\n",
      "7: 0\n",
      "8: 0\n",
      "9: 0\n",
      "10: 0\n",
      "11: 0\n",
      "12: 0\n",
      "13: 0\n",
      "14: 0\n",
      "15: 0\n",
      "16: 0\n",
      "17: 0\n",
      "18: 0\n",
      "19: 0\n",
      "20: 0\n",
      "21: 0\n",
      "22: 0\n",
      "23: 0\n",
      "24: 0\n",
      "25: 0\n",
      "26: 0\n",
      "27: 0\n",
      "28: 0\n",
      "29: 0\n",
      "30: 0\n",
      "31: 0\n",
      "32: 0\n",
      "33: 0\n",
      "34: 0\n",
      "35: 0\n",
      "36: 0\n",
      "37: 0\n",
      "38: 0\n",
      "39: 0\n",
      "40: 0\n",
      "41: 0\n",
      "42: 0\n",
      "43: 0\n",
      "44: 0\n",
      "45: 0\n",
      "46: 0\n",
      "47: 0\n",
      "48: 0\n",
      "49: 0\n",
      "50: 0\n",
      "51: 0\n",
      "52: 0\n",
      "53: 0\n",
      "54: 0\n",
      "55: 0\n",
      "56: 0\n",
      "57: 0\n",
      "58: 0\n",
      "59: 0\n",
      "60: 0\n",
      "61: 0\n",
      "62: 0\n",
      "63: 0\n",
      "64: 0\n",
      "65: 0\n",
      "66: 0\n",
      "67: 0\n",
      "68: 0\n",
      "69: 0\n",
      "70: 0\n",
      "71: 0\n",
      "72: 0\n",
      "73: 0\n",
      "74: 0\n",
      "75: 0\n",
      "76: 0\n",
      "77: 0\n",
      "78: 0\n",
      "79: 0\n",
      "80: 0\n",
      "81: 0\n",
      "82: 0\n",
      "83: 0\n",
      "84: 0\n",
      "85: 0\n",
      "86: 0\n",
      "87: 0\n",
      "88: 0\n",
      "89: 0\n",
      "90: 0\n",
      "91: 0\n",
      "92: 0\n",
      "93: 0\n",
      "94: 0\n",
      "95: 0\n",
      "96: 0\n",
      "97: 0\n",
      "98: 0\n",
      "99: 0\n",
      "100: 0\n",
      "101: 0\n",
      "102: 0\n",
      "103: 0\n",
      "104: 0\n",
      "105: 0\n",
      "106: 0\n",
      "107: 0\n",
      "108: 0\n",
      "109: 0\n",
      "110: 0\n",
      "111: 0\n",
      "112: 0\n",
      "113: 0\n",
      "114: 0\n",
      "115: 0\n",
      "116: 0\n",
      "117: 0\n",
      "118: 0\n",
      "119: 0\n",
      "120: 0\n",
      "121: 0\n",
      "122: 0\n",
      "123: 0\n",
      "124: 0\n",
      "125: 0\n",
      "126: 0\n",
      "127: 0\n",
      "128: 0\n",
      "129: 0\n",
      "130: 0\n",
      "131: 0\n",
      "132: 0\n",
      "133: 0\n",
      "134: 0\n",
      "135: 0\n",
      "136: 0\n",
      "137: 0\n",
      "138: 0\n",
      "139: 0\n",
      "140: 0\n",
      "141: 0\n",
      "142: 0\n",
      "143: 0\n",
      "144: 0\n",
      "145: 0\n",
      "146: 0\n",
      "147: 0\n",
      "148: 0\n",
      "149: 0\n",
      "150: 0\n",
      "151: 0\n",
      "152: 0\n",
      "153: 0\n",
      "154: 0\n",
      "155: 0\n",
      "156: 0\n",
      "157: 0\n",
      "158: 0\n",
      "159: 0\n",
      "160: 0\n",
      "161: 0\n",
      "162: 0\n",
      "163: 0\n",
      "164: 0\n",
      "165: 0\n",
      "166: 0\n",
      "167: 0\n",
      "168: 0\n",
      "169: 0\n",
      "170: 0\n",
      "171: 0\n",
      "172: 0\n",
      "173: 0\n",
      "174: 0\n",
      "175: 0\n",
      "176: 0\n",
      "177: 0\n",
      "178: 0\n",
      "179: 0\n",
      "180: 0\n",
      "181: 0\n",
      "182: 0\n",
      "183: 0\n",
      "184: 0\n",
      "185: 0\n",
      "186: 0\n",
      "187: 0\n",
      "188: 0\n",
      "189: 0\n",
      "190: 0\n",
      "191: 0\n",
      "192: 0\n",
      "193: 0\n",
      "194: 0\n",
      "195: 0\n",
      "196: 0\n",
      "197: 0\n",
      "198: 0\n",
      "199: 0\n",
      "200: 0\n",
      "201: 0\n",
      "202: 0\n",
      "203: 0\n",
      "204: 0\n",
      "205: 0\n",
      "206: 0\n",
      "207: 0\n",
      "208: 0\n",
      "209: 0\n",
      "210: 0\n",
      "211: 0\n",
      "212: 0\n",
      "213: 0\n",
      "214: 0\n",
      "215: 0\n",
      "216: 0\n",
      "217: 0\n",
      "218: 0\n",
      "219: 0\n",
      "220: 0\n",
      "221: 0\n",
      "222: 0\n",
      "223: 0\n",
      "224: 0\n",
      "225: 0\n",
      "226: 0\n",
      "227: 0\n",
      "228: 0\n",
      "229: 0\n",
      "230: 0\n",
      "231: 0\n",
      "232: 0\n",
      "233: 0\n",
      "234: 0\n",
      "235: 0\n",
      "236: 0\n",
      "237: 0\n",
      "238: 0\n",
      "239: 0\n",
      "240: 0\n",
      "241: 0\n",
      "242: 0\n",
      "243: 0\n",
      "244: 0\n",
      "245: 0\n",
      "246: 0\n",
      "247: 0\n",
      "248: 0\n",
      "249: 0\n",
      "250: 0\n",
      "251: 0\n",
      "252: 0\n",
      "253: 0\n",
      "254: 0\n",
      "255: 0\n",
      "256: 0\n",
      "257: 0\n",
      "258: 0\n",
      "259: 0\n",
      "260: 0\n",
      "261: 0\n",
      "262: 0\n",
      "263: 0\n",
      "264: 0\n",
      "265: 0\n",
      "266: 0\n",
      "267: 0\n",
      "268: 0\n",
      "269: 0\n",
      "270: 0\n",
      "271: 0\n",
      "272: 0\n",
      "273: 0\n",
      "274: 0\n",
      "275: 0\n",
      "276: 0\n",
      "277: 0\n",
      "278: 0\n",
      "279: 0\n",
      "280: 0\n",
      "281: 0\n",
      "282: 0\n",
      "283: 0\n",
      "284: 0\n",
      "285: 0\n",
      "286: 0\n",
      "287: 0\n",
      "288: 0\n",
      "289: 0\n",
      "290: 0\n",
      "291: 0\n",
      "292: 0\n",
      "293: 0\n",
      "294: 0\n",
      "295: 0\n",
      "296: 0\n",
      "297: 0\n",
      "298: 0\n",
      "299: 0\n",
      "300: 0\n",
      "301: 0\n",
      "302: 0\n",
      "303: 0\n",
      "304: 0\n",
      "305: 0\n",
      "306: 0\n",
      "307: 0\n",
      "308: 0\n",
      "309: 0\n",
      "310: 0\n",
      "311: 0\n",
      "312: 0\n",
      "313: 0\n",
      "314: 0\n",
      "315: 0\n",
      "316: 0\n",
      "317: 0\n",
      "318: 0\n",
      "319: 0\n",
      "320: 0\n",
      "321: 0\n",
      "322: 0\n",
      "323: 0\n",
      "324: 0\n",
      "325: 0\n",
      "326: 0\n",
      "327: 0\n",
      "328: 0\n",
      "329: 0\n",
      "330: 0\n",
      "331: 0\n",
      "332: 0\n",
      "333: 0\n",
      "334: 0\n",
      "335: 0\n",
      "336: 0\n",
      "337: 0\n",
      "338: 0\n",
      "339: 0\n",
      "340: 0\n",
      "341: 0\n",
      "342: 0\n",
      "343: 0\n",
      "344: 0\n",
      "345: 0\n",
      "346: 0\n",
      "347: 0\n",
      "348: 0\n",
      "349: 0\n",
      "350: 0\n",
      "351: 0\n",
      "352: 0\n",
      "353: 0\n",
      "354: 0\n",
      "355: 0\n",
      "356: 0\n",
      "357: 0\n",
      "358: 0\n",
      "359: 0\n",
      "360: 0\n",
      "361: 0\n",
      "362: 0\n",
      "363: 0\n",
      "364: 0\n",
      "365: 0\n",
      "366: 0\n",
      "367: 0\n",
      "368: 0\n",
      "369: 0\n",
      "370: 0\n",
      "371: 0\n",
      "372: 0\n",
      "373: 0\n",
      "374: 0\n",
      "375: 0\n",
      "376: 0\n",
      "377: 0\n",
      "378: 0\n",
      "379: 0\n",
      "380: 0\n",
      "381: 0\n",
      "382: 0\n",
      "383: 0\n",
      "384: 0\n",
      "385: 0\n",
      "386: 0\n",
      "387: 0\n",
      "388: 0\n",
      "389: 0\n",
      "390: 0\n",
      "391: 0\n",
      "392: 0\n",
      "393: 0\n",
      "394: 0\n",
      "395: 0\n",
      "396: 0\n",
      "397: 0\n",
      "398: 0\n",
      "399: 0\n",
      "400: 0\n",
      "401: 0\n",
      "402: 0\n",
      "403: 0\n",
      "404: 0\n",
      "405: 0\n",
      "406: 0\n",
      "407: 0\n",
      "408: 0\n",
      "409: 0\n",
      "410: 0\n",
      "411: 0\n",
      "412: 0\n",
      "413: 0\n",
      "414: 0\n",
      "415: 0\n",
      "416: 0\n",
      "417: 0\n",
      "418: 0\n",
      "419: 0\n",
      "420: 0\n",
      "421: 0\n",
      "422: 0\n",
      "423: 0\n",
      "424: 0\n",
      "425: 0\n",
      "426: 0\n",
      "427: 0\n",
      "428: 0\n",
      "429: 0\n",
      "430: 0\n",
      "431: 0\n",
      "432: 0\n",
      "433: 0\n",
      "434: 0\n",
      "435: 0\n",
      "436: 0\n",
      "437: 0\n",
      "438: 0\n",
      "439: 0\n",
      "440: 0\n",
      "441: 0\n",
      "442: 0\n",
      "443: 0\n",
      "444: 0\n",
      "445: 0\n",
      "446: 0\n",
      "447: 0\n",
      "448: 0\n",
      "449: 0\n",
      "450: 0\n",
      "451: 0\n",
      "452: 0\n",
      "453: 0\n",
      "454: 0\n",
      "455: 0\n",
      "456: 0\n",
      "457: 0\n",
      "458: 0\n",
      "459: 0\n",
      "460: 0\n",
      "461: 0\n",
      "462: 0\n",
      "463: 0\n",
      "464: 0\n",
      "465: 0\n",
      "466: 0\n",
      "467: 0\n",
      "468: 0\n",
      "469: 0\n",
      "470: 0\n",
      "471: 0\n",
      "472: 0\n",
      "473: 0\n",
      "474: 0\n",
      "475: 0\n",
      "476: 0\n",
      "477: 0\n",
      "478: 0\n",
      "479: 0\n",
      "480: 0\n",
      "481: 0\n",
      "482: 0\n",
      "483: 0\n",
      "484: 0\n",
      "485: 0\n",
      "486: 0\n",
      "487: 0\n",
      "488: 0\n",
      "489: 0\n",
      "490: 0\n",
      "491: 0\n",
      "492: 0\n",
      "493: 0\n",
      "494: 0\n",
      "495: 0\n",
      "496: 0\n",
      "497: 0\n",
      "498: 0\n",
      "499: 0\n",
      "500: 0\n",
      "501: 0\n",
      "502: 0\n",
      "503: 0\n",
      "504: 0\n",
      "505: 0\n",
      "506: 0\n",
      "507: 0\n",
      "508: 0\n",
      "509: 0\n",
      "510: 0\n",
      "511: 0\n",
      "512: 0\n",
      "513: 0\n",
      "514: 0\n",
      "515: 0\n",
      "516: 0\n",
      "517: 0\n",
      "518: 0\n",
      "519: 0\n",
      "520: 0\n",
      "521: 0\n",
      "522: 0\n",
      "523: 0\n",
      "524: 0\n",
      "525: 0\n",
      "526: 0\n",
      "527: 0\n",
      "528: 0\n",
      "529: 0\n",
      "530: 0\n",
      "531: 0\n",
      "532: 0\n",
      "533: 0\n",
      "534: 0\n",
      "535: 0\n",
      "536: 0\n",
      "537: 0\n",
      "538: 0\n",
      "539: 0\n",
      "540: 0\n",
      "541: 0\n",
      "542: 0\n",
      "543: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppose df_tfidf is your TF-IDF matrix (pandas DataFrame)\n",
    "# Rows = documents (e.g., D1, D2, …)\n",
    "# Columns = terms\n",
    "# Values = TF-IDF weights\n",
    "\n",
    "def fuzzy_score(doc_weights, query_tokens, operator='AND'):\n",
    "    \"\"\"\n",
    "    Compute fuzzy boolean similarity between a document and a query.\n",
    "    \n",
    "    Parameters:\n",
    "        doc_weights : dict {term: weight}\n",
    "        query_tokens : list of query terms (preprocessed)\n",
    "        operator : 'AND' | 'OR'\n",
    "    Returns:\n",
    "        float : fuzzy similarity in [0, 1]\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for t in query_tokens:\n",
    "        weights.append(doc_weights.get(t, 0))\n",
    "    \n",
    "    if not weights:\n",
    "        return 0.0\n",
    "\n",
    "    if operator == 'AND':\n",
    "        return np.min(weights)\n",
    "    elif operator == 'OR':\n",
    "        return np.max(weights)\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be 'AND' or 'OR'\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Example usage\n",
    "# ----------------------------------------------------------\n",
    "query = \"cat AND mat\"\n",
    "tokens = [t.lower() for t in query.split() if t.lower() not in ['and', 'or', 'not']]\n",
    "operator = 'AND' if 'AND' in query else 'OR'\n",
    "\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "df_tfidf = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "results = {}\n",
    "for doc in df_tfidf.index:\n",
    "    doc_vector = df_tfidf.loc[doc].to_dict()\n",
    "    score = fuzzy_score(doc_vector, tokens, operator)\n",
    "    results[doc] = round(score, 4)\n",
    "\n",
    "print(\"Fuzzy Boolean results:\")\n",
    "for doc, s in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{doc}: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6453765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: (10% AND 12%) OR (175 AND NOT 2)\n",
      "\n",
      "Doc\tScore\n",
      "D2\t0.0715\n",
      "D6\t0.0\n",
      "D4\t0.0\n",
      "D1\t0.0\n",
      "D5\t0.0\n",
      "D3\t0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD THE INVERTED INDEX FILE\n",
    "# Format: <Term> <Doc> <Freq> <TF-IDF>\n",
    "# ------------------------------------------------------------\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "\n",
    "# Read tab-separated txt file \n",
    "data = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "\n",
    "# Build a document-term matrix {doc: {term: tfidf}}\n",
    "doc_dict = defaultdict(dict)\n",
    "for _, row in data.iterrows():\n",
    "    doc_dict[row[\"doc\"]][str(row[\"term\"]).lower()] = float(row[\"tfidf\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. FUZZY BOOLEAN EVALUATION FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def fuzzy_and(a, b): return min(a, b)\n",
    "def fuzzy_or(a, b): return max(a, b)\n",
    "def fuzzy_not(a): return 1 - a\n",
    "\n",
    "def get_weight(doc, term):\n",
    "    \"\"\"Return TF-IDF weight of term in doc, or 0 if missing.\"\"\"\n",
    "    return doc_dict[doc].get(term.lower(), 0.0)\n",
    "\n",
    "def eval_fuzzy(query, doc):\n",
    "    \"\"\"Evaluate fuzzy boolean query for one document.\"\"\"\n",
    "    # Add spaces around parentheses\n",
    "    query = query.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    tokens = query.split()\n",
    "    \n",
    "    def parse(tokens):\n",
    "        stack = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tok = tokens[i].lower()\n",
    "            if tok == \"(\":\n",
    "                # find matching parenthesis\n",
    "                depth = 0\n",
    "                j = i\n",
    "                while j < len(tokens):\n",
    "                    if tokens[j] == \"(\":\n",
    "                        depth += 1\n",
    "                    elif tokens[j] == \")\":\n",
    "                        depth -= 1\n",
    "                        if depth == 0:\n",
    "                            break\n",
    "                    j += 1\n",
    "                val = parse(tokens[i + 1:j])\n",
    "                stack.append(val)\n",
    "                i = j\n",
    "            elif tok == \"and\":\n",
    "                stack.append(\"AND\")\n",
    "            elif tok == \"or\":\n",
    "                stack.append(\"OR\")\n",
    "            elif tok == \"not\":\n",
    "                # next token is negated\n",
    "                next_term = tokens[i + 1].lower()\n",
    "                w = fuzzy_not(get_weight(doc, next_term))\n",
    "                stack.append(w)\n",
    "                i += 1\n",
    "            else:\n",
    "                stack.append(get_weight(doc, tok))\n",
    "            i += 1\n",
    "        \n",
    "        # Evaluate AND first, then OR\n",
    "        while \"AND\" in stack:\n",
    "            idx = stack.index(\"AND\")\n",
    "            res = fuzzy_and(stack[idx - 1], stack[idx + 1])\n",
    "            stack = stack[:idx - 1] + [res] + stack[idx + 2:]\n",
    "        while \"OR\" in stack:\n",
    "            idx = stack.index(\"OR\")\n",
    "            res = fuzzy_or(stack[idx - 1], stack[idx + 1])\n",
    "            stack = stack[:idx - 1] + [res] + stack[idx + 2:]\n",
    "        return stack[0]\n",
    "\n",
    "    return parse(tokens)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. RUN A QUERY ON ALL DOCUMENTS\n",
    "# ------------------------------------------------------------\n",
    "def fuzzy_search(query):\n",
    "    results = {}\n",
    "    for doc in doc_dict.keys():\n",
    "        score = eval_fuzzy(query, doc)\n",
    "        results[doc] = round(score, 4)\n",
    "    # Sort descending by score\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST\n",
    "# ------------------------------------------------------------\n",
    "query = \"(10% AND 12%) OR (175 AND NOT 2)\"\n",
    "results = fuzzy_search(query)\n",
    "\n",
    "print(f\"\\n🔍 Query: {query}\\n\")\n",
    "print(\"Doc\\tScore\")\n",
    "for doc, score in results.items():\n",
    "    print(f\"{doc}\\t{score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd3da3",
   "metadata": {},
   "source": [
    "## Extended Boolean Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d99857",
   "metadata": {},
   "source": [
    "## 🔹 1. Concept Recap\n",
    "\n",
    "The **Extended Boolean Model** replaces the strict Boolean **AND / OR** logic with continuous functions that measure **how much a document satisfies a query**.\n",
    "\n",
    "It uses the **p-norm operator**, where the parameter \\( p \\) controls how strict or relaxed the matching is:\n",
    "\n",
    "| p value | Behavior |\n",
    "|:--------:|:----------|\n",
    "| \\( p \\to \\infty \\) | strict Boolean (perfect AND/OR) |\n",
    "| \\( p = 1 \\) | loose, soft matching (closer to vector model) |\n",
    "| **Typical value** | between 2 and 5 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Formulas\n",
    "\n",
    "Let \\( w_{d_i} \\) be the **TF-IDF weight** of term *i* in document *d*, normalized to [0, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 AND Query\n",
    "\n",
    "\\[\n",
    "S_{AND}(d, q) = \\left( \\frac{1}{n} \\sum_{i=1}^{n} (w_{d_i})^p \\right)^{\\frac{1}{p}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 OR Query\n",
    "\n",
    "\\[\n",
    "S_{OR}(d, q) = 1 - \\left( \\frac{1}{n} \\sum_{i=1}^{n} (1 - w_{d_i})^p \\right)^{\\frac{1}{p}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "where  \n",
    "\\( n \\) = number of terms in the query,  \n",
    "and \\( p \\) = the **p-norm parameter** controlling the **strictness of matching**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Intuition:**\n",
    "- When \\( p \\) is large → behavior approaches strict Boolean logic.  \n",
    "- When \\( p \\) is small → more flexible, similar to vector-space similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd59b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Extended Boolean Query: 10% AND 12% (p=2)\n",
      "\n",
      "Doc\tScore\n",
      "D2\t0.1011\n",
      "D4\t0.0531\n",
      "D6\t0.0\n",
      "D1\t0.0\n",
      "D5\t0.0\n",
      "D3\t0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD THE INVERTED INDEX\n",
    "# ------------------------------------------------------------\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "\n",
    "data = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "\n",
    "# Build structure: {doc: {term: tfidf}}\n",
    "doc_dict = defaultdict(dict)\n",
    "for _, row in data.iterrows():\n",
    "    doc_dict[row[\"doc\"]][str(row[\"term\"]).lower()] = float(row[\"tfidf\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. EXTENDED BOOLEAN MODEL FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def extended_boolean_score(doc_weights, query_terms, operator=\"AND\", p=2):\n",
    "    \"\"\"\n",
    "    Compute the Extended Boolean model score for one document.\n",
    "    - doc_weights: dict {term: weight}\n",
    "    - query_terms: list of query tokens (strings)\n",
    "    - operator: 'AND' or 'OR'\n",
    "    - p: float, p-norm parameter\n",
    "    \"\"\"\n",
    "    # Extract weights for query terms (default 0 if term not in doc)\n",
    "    w = np.array([doc_weights.get(term.lower(), 0.0) for term in query_terms])\n",
    "    n = len(w)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if operator.upper() == \"AND\":\n",
    "        return (np.sum(w ** p) / n) ** (1 / p)\n",
    "    elif operator.upper() == \"OR\":\n",
    "        return 1 - ((np.sum((1 - w) ** p) / n) ** (1 / p))\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be 'AND' or 'OR'\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. QUERY EXECUTION\n",
    "# ------------------------------------------------------------\n",
    "def extended_boolean_search(query, p=2):\n",
    "    \"\"\"\n",
    "    Execute an AND/OR query across all documents using the Extended Boolean Model.\n",
    "    \"\"\"\n",
    "    # Detect operator\n",
    "    query_lower = query.lower()\n",
    "    if \" and \" in query_lower:\n",
    "        operator = \"AND\"\n",
    "        terms = [t.strip() for t in query_lower.split(\"and\")]\n",
    "    elif \" or \" in query_lower:\n",
    "        operator = \"OR\"\n",
    "        terms = [t.strip() for t in query_lower.split(\"or\")]\n",
    "    else:\n",
    "        operator = \"AND\"\n",
    "        terms = [query_lower.strip()]\n",
    "    \n",
    "    # Compute scores for all docs\n",
    "    results = {}\n",
    "    for doc, weights in doc_dict.items():\n",
    "        score = extended_boolean_score(weights, terms, operator=operator, p=p)\n",
    "        results[doc] = round(score, 4)\n",
    "\n",
    "    # Sort results by descending score\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST\n",
    "# ------------------------------------------------------------\n",
    "query = \"10% AND 12%\"\n",
    "p = 2  # try 1, 2, or higher for stricter matching\n",
    "\n",
    "results = extended_boolean_search(query, p=p)\n",
    "\n",
    "print(f\"\\n🔍 Extended Boolean Query: {query} (p={p})\\n\")\n",
    "print(\"Doc\\tScore\")\n",
    "for doc, score in results.items():\n",
    "    print(f\"{doc}\\t{score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
