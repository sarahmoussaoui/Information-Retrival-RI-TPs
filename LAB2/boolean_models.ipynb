{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0706832",
   "metadata": {},
   "source": [
    "# Task 1 ‚Äì Boolean Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a998963",
   "metadata": {},
   "source": [
    "## Classic boolean model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d600bc2",
   "metadata": {},
   "source": [
    "Each term is binary (present or not).\n",
    "\n",
    "Retrieve only documents that exactly satisfy the Boolean expression.\n",
    "\n",
    "Operators: AND, OR, NOT.\n",
    "\n",
    "Example query: q = (query AND reformulation) OR (Language AND model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69aebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(query AND reformulation) OR (language AND model)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec43351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classic Boolean Model Results:\n",
      "Query: (query AND reformulation) OR (language AND model)\n",
      "Retrieved documents: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ---------- 1. Preprocessing ----------\n",
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stops]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------- 2. Load Inverted Index ----------\n",
    "def load_inverted_index(filepath):\n",
    "    inverted = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                term, doc_id = parts[0], parts[1]\n",
    "                inverted.setdefault(term, set()).add(doc_id)\n",
    "    return inverted\n",
    "\n",
    "\n",
    "# ---------- 3. Evaluate Boolean Query ----------\n",
    "def evaluate_boolean_query(query, inverted_index, all_docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # Extract all candidate words (ignoring AND/OR/NOT and parentheses)\n",
    "    raw_tokens = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    unique_tokens = set(raw_tokens) - {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "    # Start with the original expression\n",
    "    expression = query\n",
    "\n",
    "    # For each token, find its stemmed version and replace it\n",
    "    for token in unique_tokens:\n",
    "        if token.lower() in stops:\n",
    "            continue\n",
    "        stemmed = stemmer.stem(token.lower())\n",
    "        docs = inverted_index.get(stemmed, set())\n",
    "        expression = re.sub(rf'\\b{token}\\b', f\"set({list(docs)})\", expression, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace logical operators with Python equivalents\n",
    "    expression = re.sub(r\"\\bAND\\b\", \"&\", expression, flags=re.IGNORECASE)\n",
    "    expression = re.sub(r\"\\bOR\\b\", \"|\", expression, flags=re.IGNORECASE)\n",
    "    expression = re.sub(r\"\\bNOT\\b\", \"all_docs -\", expression, flags=re.IGNORECASE)\n",
    "\n",
    "    # Evaluate expression safely\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": None}, {\"all_docs\": all_docs, \"set\": set})\n",
    "    except Exception as e:\n",
    "        print(\"Error in query:\", e)\n",
    "        print(\"Expression after replacements:\", expression)\n",
    "        return set()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------- 4. Example Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    inverted_index = load_inverted_index(\"results/inverted_index_weighted.txt\")\n",
    "    all_docs = {f\"D{i}\" for i in range(1, 7)}\n",
    "\n",
    "    relevant_docs = evaluate_boolean_query(query, inverted_index, all_docs)\n",
    "\n",
    "    print(\"\\nClassic Boolean Model Results:\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Retrieved documents:\", sorted(relevant_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e7c1e",
   "metadata": {},
   "source": [
    "## Fuzzy boolean model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76864765",
   "metadata": {},
   "source": [
    "### üîπ 1. Concept Recap\n",
    "\n",
    "The **Fuzzy Boolean Model** is a hybrid between:\n",
    "\n",
    "- the **Boolean model** (logical operators `AND`, `OR`, `NOT`)  \n",
    "- and the **Vector model** (graded, real-valued similarities instead of strict true/false).\n",
    "\n",
    "Each term weight (e.g., **TF-IDF**) is treated as a **degree of membership** in the interval **[0, 1]**, not binary.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Core idea\n",
    "\n",
    "For each query term \\( t \\) and document \\( d \\):\n",
    "\n",
    "\\[\n",
    "w_{t,d} = \\text{TF-IDF weight of term } t \\text{ in document } d\n",
    "\\]\n",
    "\n",
    "Each document‚Äôs relevance to a query is computed using **fuzzy logic operators**:\n",
    "\n",
    "- **AND ‚Üí** use `min`\n",
    "- **OR ‚Üí** use `max`\n",
    "- **NOT ‚Üí** use `1 - weight`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Fuzzy Boolean Operators\n",
    "\n",
    "| Operator | Boolean | Fuzzy Equivalent | Formula |\n",
    "|:---------:|:--------:|:----------------:|:--------:|\n",
    "| **AND** | ‚àß | min | \\( S_{AND}(d,q) = \\min(w_{t1,d}, w_{t2,d}) \\) |\n",
    "| **OR** | ‚à® | max | \\( S_{OR}(d,q) = \\max(w_{t1,d}, w_{t2,d}) \\) |\n",
    "| **NOT** | ¬¨ | complement | \\( S_{NOT}(d,q) = 1 - w_{t,d} \\) |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Note:**  \n",
    "For multi-term queries, you can combine these operators **recursively** to compute the final fuzzy relevance score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fd92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m operator = \u001b[33m'\u001b[39m\u001b[33mAND\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mAND\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mOR\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     41\u001b[39m inverted_path = \u001b[33m\"\u001b[39m\u001b[33mresults/inverted_index_weighted.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m df_tfidf = \u001b[43mpd\u001b[49m.read_csv(inverted_path, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mNone\u001b[39;00m, names=[\u001b[33m\"\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdoc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfreq\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     43\u001b[39m results = {}\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m df_tfidf.index:\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Suppose df_tfidf is your TF-IDF matrix (pandas DataFrame)\n",
    "# Rows = documents (e.g., D1, D2, ‚Ä¶)\n",
    "# Columns = terms\n",
    "# Values = TF-IDF weights\n",
    "\n",
    "def fuzzy_score(doc_weights, query_tokens, operator='AND'):\n",
    "    \"\"\"\n",
    "    Compute fuzzy boolean similarity between a document and a query.\n",
    "    \n",
    "    Parameters:\n",
    "        doc_weights : dict {term: weight}\n",
    "        query_tokens : list of query terms (preprocessed)\n",
    "        operator : 'AND' | 'OR'\n",
    "    Returns:\n",
    "        float : fuzzy similarity in [0, 1]\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for t in query_tokens:\n",
    "        weights.append(doc_weights.get(t, 0))\n",
    "    \n",
    "    if not weights:\n",
    "        return 0.0\n",
    "\n",
    "    if operator == 'AND':\n",
    "        return np.min(weights)\n",
    "    elif operator == 'OR':\n",
    "        return np.max(weights)\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be 'AND' or 'OR'\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Example usage\n",
    "# ----------------------------------------------------------\n",
    "query = \"cat AND mat\"\n",
    "tokens = [t.lower() for t in query.split() if t.lower() not in ['and', 'or', 'not']]\n",
    "operator = 'AND' if 'AND' in query else 'OR'\n",
    "\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "df_tfidf = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "results = {}\n",
    "for doc in df_tfidf.index:\n",
    "    doc_vector = df_tfidf.loc[doc].to_dict()\n",
    "    score = fuzzy_score(doc_vector, tokens, operator)\n",
    "    results[doc] = round(score, 4)\n",
    "\n",
    "print(\"Fuzzy Boolean results:\")\n",
    "for doc, s in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{doc}: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6453765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query: (10% AND 12%) OR (175 AND NOT 2)\n",
      "\n",
      "Doc\tScore\n",
      "D2\t0.0715\n",
      "D6\t0.0\n",
      "D4\t0.0\n",
      "D1\t0.0\n",
      "D5\t0.0\n",
      "D3\t0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD THE INVERTED INDEX FILE\n",
    "# Format: <Term> <Doc> <Freq> <TF-IDF>\n",
    "# ------------------------------------------------------------\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "\n",
    "# Read tab-separated txt file \n",
    "data = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "\n",
    "# Build a document-term matrix {doc: {term: tfidf}}\n",
    "doc_dict = defaultdict(dict)\n",
    "for _, row in data.iterrows():\n",
    "    doc_dict[row[\"doc\"]][str(row[\"term\"]).lower()] = float(row[\"tfidf\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. FUZZY BOOLEAN EVALUATION FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def fuzzy_and(a, b): return min(a, b)\n",
    "def fuzzy_or(a, b): return max(a, b)\n",
    "def fuzzy_not(a): return 1 - a\n",
    "\n",
    "def get_weight(doc, term):\n",
    "    \"\"\"Return TF-IDF weight of term in doc, or 0 if missing.\"\"\"\n",
    "    return doc_dict[doc].get(term.lower(), 0.0)\n",
    "\n",
    "def eval_fuzzy(query, doc):\n",
    "    \"\"\"Evaluate fuzzy boolean query for one document.\"\"\"\n",
    "    # Add spaces around parentheses\n",
    "    query = query.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    tokens = query.split()\n",
    "    \n",
    "    def parse(tokens):\n",
    "        stack = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tok = tokens[i].lower()\n",
    "            if tok == \"(\":\n",
    "                # find matching parenthesis\n",
    "                depth = 0\n",
    "                j = i\n",
    "                while j < len(tokens):\n",
    "                    if tokens[j] == \"(\":\n",
    "                        depth += 1\n",
    "                    elif tokens[j] == \")\":\n",
    "                        depth -= 1\n",
    "                        if depth == 0:\n",
    "                            break\n",
    "                    j += 1\n",
    "                val = parse(tokens[i + 1:j])\n",
    "                stack.append(val)\n",
    "                i = j\n",
    "            elif tok == \"and\":\n",
    "                stack.append(\"AND\")\n",
    "            elif tok == \"or\":\n",
    "                stack.append(\"OR\")\n",
    "            elif tok == \"not\":\n",
    "                # next token is negated\n",
    "                next_term = tokens[i + 1].lower()\n",
    "                w = fuzzy_not(get_weight(doc, next_term))\n",
    "                stack.append(w)\n",
    "                i += 1\n",
    "            else:\n",
    "                stack.append(get_weight(doc, tok))\n",
    "            i += 1\n",
    "        \n",
    "        # Evaluate AND first, then OR\n",
    "        while \"AND\" in stack:\n",
    "            idx = stack.index(\"AND\")\n",
    "            res = fuzzy_and(stack[idx - 1], stack[idx + 1])\n",
    "            stack = stack[:idx - 1] + [res] + stack[idx + 2:]\n",
    "        while \"OR\" in stack:\n",
    "            idx = stack.index(\"OR\")\n",
    "            res = fuzzy_or(stack[idx - 1], stack[idx + 1])\n",
    "            stack = stack[:idx - 1] + [res] + stack[idx + 2:]\n",
    "        return stack[0]\n",
    "\n",
    "    return parse(tokens)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. RUN A QUERY ON ALL DOCUMENTS\n",
    "# ------------------------------------------------------------\n",
    "def fuzzy_search(query):\n",
    "    results = {}\n",
    "    for doc in doc_dict.keys():\n",
    "        score = eval_fuzzy(query, doc)\n",
    "        results[doc] = round(score, 4)\n",
    "    # Sort descending by score\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST\n",
    "# ------------------------------------------------------------\n",
    "query = \"(10% AND 12%) OR (175 AND NOT 2)\"\n",
    "results = fuzzy_search(query)\n",
    "\n",
    "print(f\"\\nüîç Query: {query}\\n\")\n",
    "print(\"Doc\\tScore\")\n",
    "for doc, score in results.items():\n",
    "    print(f\"{doc}\\t{score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd3da3",
   "metadata": {},
   "source": [
    "# Extended Boolean Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d99857",
   "metadata": {},
   "source": [
    "## üîπ 1. Concept Recap\n",
    "\n",
    "The **Extended Boolean Model** replaces the strict Boolean **AND / OR** logic with continuous functions that measure **how much a document satisfies a query**.\n",
    "\n",
    "It uses the **p-norm operator**, where the parameter \\( p \\) controls how strict or relaxed the matching is:\n",
    "\n",
    "| p value | Behavior |\n",
    "|:--------:|:----------|\n",
    "| \\( p \\to \\infty \\) | strict Boolean (perfect AND/OR) |\n",
    "| \\( p = 1 \\) | loose, soft matching (closer to vector model) |\n",
    "| **Typical value** | between 2 and 5 |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Formulas\n",
    "\n",
    "Let \\( w_{d_i} \\) be the **TF-IDF weight** of term *i* in document *d*, normalized to [0, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ AND Query\n",
    "\n",
    "\\[\n",
    "S_{AND}(d, q) = \\left( \\frac{1}{n} \\sum_{i=1}^{n} (w_{d_i})^p \\right)^{\\frac{1}{p}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ OR Query\n",
    "\n",
    "\\[\n",
    "S_{OR}(d, q) = 1 - \\left( \\frac{1}{n} \\sum_{i=1}^{n} (1 - w_{d_i})^p \\right)^{\\frac{1}{p}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "where  \n",
    "\\( n \\) = number of terms in the query,  \n",
    "and \\( p \\) = the **p-norm parameter** controlling the **strictness of matching**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Intuition:**\n",
    "- When \\( p \\) is large ‚Üí behavior approaches strict Boolean logic.  \n",
    "- When \\( p \\) is small ‚Üí more flexible, similar to vector-space similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd59b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Extended Boolean Query: 10% AND 12% (p=2)\n",
      "\n",
      "Doc\tScore\n",
      "D2\t0.1011\n",
      "D4\t0.0531\n",
      "D6\t0.0\n",
      "D1\t0.0\n",
      "D5\t0.0\n",
      "D3\t0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD THE INVERTED INDEX\n",
    "# ------------------------------------------------------------\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "\n",
    "data = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "\n",
    "# Build structure: {doc: {term: tfidf}}\n",
    "doc_dict = defaultdict(dict)\n",
    "for _, row in data.iterrows():\n",
    "    doc_dict[row[\"doc\"]][str(row[\"term\"]).lower()] = float(row[\"tfidf\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. EXTENDED BOOLEAN MODEL FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def extended_boolean_score(doc_weights, query_terms, operator=\"AND\", p=2):\n",
    "    \"\"\"\n",
    "    Compute the Extended Boolean model score for one document.\n",
    "    - doc_weights: dict {term: weight}\n",
    "    - query_terms: list of query tokens (strings)\n",
    "    - operator: 'AND' or 'OR'\n",
    "    - p: float, p-norm parameter\n",
    "    \"\"\"\n",
    "    # Extract weights for query terms (default 0 if term not in doc)\n",
    "    w = np.array([doc_weights.get(term.lower(), 0.0) for term in query_terms])\n",
    "    n = len(w)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if operator.upper() == \"AND\":\n",
    "        return (np.sum(w ** p) / n) ** (1 / p)\n",
    "    elif operator.upper() == \"OR\":\n",
    "        return 1 - ((np.sum((1 - w) ** p) / n) ** (1 / p))\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be 'AND' or 'OR'\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. QUERY EXECUTION\n",
    "# ------------------------------------------------------------\n",
    "def extended_boolean_search(query, p=2):\n",
    "    \"\"\"\n",
    "    Execute an AND/OR query across all documents using the Extended Boolean Model.\n",
    "    \"\"\"\n",
    "    # Detect operator\n",
    "    query_lower = query.lower()\n",
    "    if \" and \" in query_lower:\n",
    "        operator = \"AND\"\n",
    "        terms = [t.strip() for t in query_lower.split(\"and\")]\n",
    "    elif \" or \" in query_lower:\n",
    "        operator = \"OR\"\n",
    "        terms = [t.strip() for t in query_lower.split(\"or\")]\n",
    "    else:\n",
    "        operator = \"AND\"\n",
    "        terms = [query_lower.strip()]\n",
    "    \n",
    "    # Compute scores for all docs\n",
    "    results = {}\n",
    "    for doc, weights in doc_dict.items():\n",
    "        score = extended_boolean_score(weights, terms, operator=operator, p=p)\n",
    "        results[doc] = round(score, 4)\n",
    "\n",
    "    # Sort results by descending score\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST\n",
    "# ------------------------------------------------------------\n",
    "query = \"10% AND 12%\"\n",
    "p = 2  # try 1, 2, or higher for stricter matching\n",
    "\n",
    "results = extended_boolean_search(query, p=p)\n",
    "\n",
    "print(f\"\\nüîç Extended Boolean Query: {query} (p={p})\\n\")\n",
    "print(\"Doc\\tScore\")\n",
    "for doc, score in results.items():\n",
    "    print(f\"{doc}\\t{score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
