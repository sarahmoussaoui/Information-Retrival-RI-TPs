{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0706832",
   "metadata": {},
   "source": [
    "# Task 1 – Boolean Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a998963",
   "metadata": {},
   "source": [
    "## Classic boolean model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d600bc2",
   "metadata": {},
   "source": [
    "Each term is binary (present or not).\n",
    "\n",
    "Retrieve only documents that exactly satisfy the Boolean expression.\n",
    "\n",
    "Operators: AND, OR, NOT.\n",
    "\n",
    "Example query: q = (query AND reformulation) OR (Language AND model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69aebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(query AND reformulation) OR (language AND model)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bec43351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classic Boolean Model Results:\n",
      "Query: (query AND reformulation) OR (language AND model)\n",
      "Retrieved documents: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ---------- 1. Preprocessing ----------\n",
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stops]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------- 2. Load Inverted Index ----------\n",
    "def load_inverted_index(filepath):\n",
    "    inverted = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                term, doc_id = parts[0], parts[1]\n",
    "                inverted.setdefault(term, set()).add(doc_id)\n",
    "    return inverted\n",
    "\n",
    "\n",
    "# ---------- 3. Evaluate Boolean Query ----------\n",
    "def evaluate_boolean_query(query, inverted_index, all_docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # Extract all candidate words (ignoring AND/OR/NOT and parentheses)\n",
    "    raw_tokens = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    unique_tokens = set(raw_tokens) - {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "    # Start with the original expression\n",
    "    expression = query\n",
    "\n",
    "    # For each token, find its stemmed version and replace it\n",
    "    for token in unique_tokens:\n",
    "        if token.lower() in stops:\n",
    "            continue\n",
    "        stemmed = stemmer.stem(token.lower())\n",
    "        docs = inverted_index.get(stemmed, set())\n",
    "        expression = re.sub(rf'\\b{token}\\b', f\"set({list(docs)})\", expression, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace logical operators with Python equivalents\n",
    "    expression = re.sub(r\"\\bAND\\b\", \"&\", expression, flags=re.IGNORECASE)\n",
    "    expression = re.sub(r\"\\bOR\\b\", \"|\", expression, flags=re.IGNORECASE)\n",
    "    expression = re.sub(r\"\\bNOT\\b\", \"all_docs -\", expression, flags=re.IGNORECASE)\n",
    "\n",
    "    # Evaluate expression safely\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": None}, {\"all_docs\": all_docs, \"set\": set})\n",
    "    except Exception as e:\n",
    "        print(\"Error in query:\", e)\n",
    "        print(\"Expression after replacements:\", expression)\n",
    "        return set()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------- 4. Example Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    inverted_index = load_inverted_index(\"results/inverted_index_weighted.txt\")\n",
    "    all_docs = {f\"D{i}\" for i in range(1, 7)}\n",
    "\n",
    "    relevant_docs = evaluate_boolean_query(query, inverted_index, all_docs)\n",
    "\n",
    "    print(\"\\nClassic Boolean Model Results:\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Retrieved documents:\", sorted(relevant_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e7c1e",
   "metadata": {},
   "source": [
    "## Fuzzy boolean model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76864765",
   "metadata": {},
   "source": [
    "### 🔹 1. Concept Recap\n",
    "\n",
    "The **Fuzzy Boolean Model** is a hybrid between:\n",
    "\n",
    "- the **Boolean model** (logical operators `AND`, `OR`, `NOT`)  \n",
    "- and the **Vector model** (graded, real-valued similarities instead of strict true/false).\n",
    "\n",
    "Each term weight (e.g., **TF-IDF**) is treated as a **degree of membership** in the interval **[0, 1]**, not binary.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 Core idea\n",
    "\n",
    "For each query term \\( t \\) and document \\( d \\):\n",
    "\n",
    "\\[\n",
    "w_{t,d} = \\text{TF-IDF weight of term } t \\text{ in document } d\n",
    "\\]\n",
    "\n",
    "Each document’s relevance to a query is computed using **fuzzy logic operators**:\n",
    "\n",
    "- **AND →** use `min`\n",
    "- **OR →** use `max`\n",
    "- **NOT →** use `1 - weight`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Fuzzy Boolean Operators\n",
    "\n",
    "| Operator | Boolean | Fuzzy Equivalent | Formula |\n",
    "|:---------:|:--------:|:----------------:|:--------:|\n",
    "| **AND** | ∧ | min | \\( S_{AND}(d,q) = \\min(w_{t1,d}, w_{t2,d}) \\) |\n",
    "| **OR** | ∨ | max | \\( S_{OR}(d,q) = \\max(w_{t1,d}, w_{t2,d}) \\) |\n",
    "| **NOT** | ¬ | complement | \\( S_{NOT}(d,q) = 1 - w_{t,d} \\) |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Note:**  \n",
    "For multi-term queries, you can combine these operators **recursively** to compute the final fuzzy relevance score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971fd92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classic] Requête : (query AND reformulation) OR (language AND model)\n",
      "[Classic] Tokens  : ['queri', 'AND', 'reformul', 'OR', 'languag', 'AND', 'model']\n",
      "[Classic] RPN     : ['queri', 'reformul', 'AND', 'languag', 'model', 'AND', 'OR']\n",
      "\n",
      "[Classic] Détail des étapes:\n",
      "- Étape 1: Terme 'queri' -> docs {'D2', 'D5', 'D4', 'D1'}\n",
      "- Étape 2: Terme 'reformul' -> docs {'D4', 'D1'}\n",
      "- Étape 3: AND -> {'D2', 'D5', 'D4', 'D1'} ∩ {'D4', 'D1'}\n",
      "= {'D4', 'D1'}\n",
      "- Étape 4: Terme 'languag' -> docs {'D3', 'D1', 'D2', 'D6', 'D4', 'D5'}\n",
      "- Étape 5: Terme 'model' -> docs {'D3', 'D1', 'D2', 'D6', 'D4', 'D5'}\n",
      "- Étape 6: AND -> {'D3', 'D1', 'D2', 'D6', 'D4', 'D5'} ∩ {'D3', 'D1', 'D2', 'D6', 'D4', 'D5'}\n",
      "= {'D3', 'D2', 'D6', 'D4', 'D5', 'D1'}\n",
      "- Étape 7: OR  -> {'D4', 'D1'} ∪ {'D3', 'D2', 'D6', 'D4', 'D5', 'D1'}\n",
      "= {'D3', 'D2', 'D6', 'D4', 'D5', 'D1'}\n",
      "\n",
      "[Classic] Résultat final (docs):\n",
      "['D1', 'D2', 'D3', 'D4', 'D5', 'D6']\n",
      "\n",
      "[Fuzzy] Requête : (query AND reformulation) OR (language AND model)\n",
      "[Fuzzy] Tokens  : ['queri', 'AND', 'reformul', 'OR', 'languag', 'AND', 'model']\n",
      "[Fuzzy] RPN     : ['queri', 'reformul', 'AND', 'languag', 'model', 'AND', 'OR']\n",
      "\n",
      "[Fuzzy] Détail des étapes:\n",
      "- Étape 1: Terme 'queri' -> RSV(d, t) (normalisé), 4 docs\n",
      "- Étape 2: Terme 'reformul' -> RSV(d, t) (normalisé), 2 docs\n",
      "- Étape 3: AND -> min(scores)\n",
      "- Étape 4: Terme 'languag' -> RSV(d, t) (normalisé), 6 docs\n",
      "- Étape 5: Terme 'model' -> RSV(d, t) (normalisé), 6 docs\n",
      "- Étape 6: AND -> min(scores)\n",
      "- Étape 7: OR  -> max(scores)\n",
      "\n",
      "[Fuzzy] Top documents (doc, RSV):\n",
      "D4\t0.2676\n",
      "D1\t0.2388\n",
      "D5\t0.1003\n",
      "D2\t0.0502\n",
      "D3\t0.0376\n",
      "D6\t0.0376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('D4', 0.267582),\n",
       " ('D1', 0.238764),\n",
       " ('D5', 0.100343),\n",
       " ('D2', 0.050172),\n",
       " ('D3', 0.037629),\n",
       " ('D6', 0.037629)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "base_path = r\"results/\"\n",
    "inverted_index_file = os.path.join(base_path, \"inverted_index_weighted.txt\")\n",
    "\n",
    "\n",
    "inverted_index = {}\n",
    "all_docs = set()\n",
    "with open(inverted_index_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        term, doc, freq, weight = parts[0], parts[1], int(parts[2]), float(parts[3])\n",
    "        inverted_index.setdefault(term, []).append((doc, freq, weight))\n",
    "        all_docs.add(doc)\n",
    "N = len(all_docs)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'(?:[A-Za-z]\\.)+|[A-Za-z]+(?:[-@]\\d+(?:\\.\\d+)?)|\\d+(?:[:\\.,\\-]\\d+)*%?|[A-Za-z]+')\n",
    "stop_en = set(stopwords.words('english'))\n",
    "BOOL_OPS = {'AND', 'OR', 'NOT', '(', ')'}\n",
    "\n",
    "\n",
    "def preprocess_query(qtext: str):\n",
    "    raw = tokenizer.tokenize(qtext.lower())\n",
    "    out = []\n",
    "    for tok in raw:\n",
    "        up = tok.upper()\n",
    "        if up in BOOL_OPS:\n",
    "            out.append(up)\n",
    "        else:\n",
    "            if tok in stop_en:\n",
    "                continue\n",
    "            out.append(ps.stem(tok))\n",
    "    return out\n",
    "\n",
    "\n",
    "PRECEDENCE = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "def to_rpn(tokens):\n",
    "    output = []\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t in BOOL_OPS and t not in {'(', ')'}:\n",
    "            while stack and stack[-1] in PRECEDENCE and PRECEDENCE[stack[-1]] >= PRECEDENCE[t]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(t)\n",
    "        elif t == '(':\n",
    "            stack.append(t)\n",
    "        elif t == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            if stack and stack[-1] == '(':\n",
    "                stack.pop()\n",
    "        else:\n",
    "            output.append(t)\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "\n",
    "def postings_set(term):\n",
    "    return set(d for d, _, _ in inverted_index.get(term, []))\n",
    "\n",
    "\n",
    "def term_score_vector(term):\n",
    "   \n",
    "    docs = inverted_index.get(term, [])\n",
    "    if not docs:\n",
    "        return {}\n",
    "    return {d: w for d, _, w in docs}\n",
    "\n",
    "\n",
    "\n",
    "def eval_classic_rpn(rpn):\n",
    "    stack = []\n",
    "    for t in rpn:\n",
    "        if t == 'NOT':\n",
    "            a = stack.pop() if stack else set()\n",
    "            stack.append(all_docs - a)\n",
    "        elif t == 'AND':\n",
    "            b = stack.pop() if stack else set()\n",
    "            a = stack.pop() if stack else set()\n",
    "            stack.append(a & b)\n",
    "        elif t == 'OR':\n",
    "            b = stack.pop() if stack else set()\n",
    "            a = stack.pop() if stack else set()\n",
    "            stack.append(a | b)\n",
    "        else:\n",
    "            stack.append(postings_set(t))\n",
    "    return stack.pop() if stack else set()\n",
    "\n",
    "\n",
    "def eval_fuzzy_rpn(rpn):\n",
    "    stack = []\n",
    "    for t in rpn:\n",
    "        if t == 'NOT':\n",
    "            a = stack.pop() if stack else {}\n",
    "            stack.append({d: 1.0 - a.get(d, 0.0) for d in all_docs})\n",
    "        elif t == 'AND':\n",
    "            b = stack.pop() if stack else {}\n",
    "            a = stack.pop() if stack else {}\n",
    "            stack.append({d: min(a.get(d, 0.0), b.get(d, 0.0)) for d in all_docs})\n",
    "        elif t == 'OR':\n",
    "            b = stack.pop() if stack else {}\n",
    "            a = stack.pop() if stack else {}\n",
    "            stack.append({d: max(a.get(d, 0.0), b.get(d, 0.0)) for d in all_docs})\n",
    "        else:\n",
    "            stack.append(term_score_vector(t))\n",
    "    return stack.pop() if stack else {}\n",
    "\n",
    "\n",
    "def run_classic_verbose(query_text):\n",
    "    toks = preprocess_query(query_text)\n",
    "    print(\"\\n[Classic] Requête :\", query_text)\n",
    "    print(\"[Classic] Tokens  :\", toks)\n",
    "    rpn = to_rpn(toks)\n",
    "    print(\"[Classic] RPN     :\", rpn)\n",
    "\n",
    "    stack = []\n",
    "    steps = []\n",
    "    for t in rpn:\n",
    "        if t == 'NOT':\n",
    "            a = stack.pop() if stack else set()\n",
    "            result = all_docs - a\n",
    "            steps.append(f\"NOT -> Complément de {a}\\n= {result}\")\n",
    "            stack.append(result)\n",
    "        elif t == 'AND':\n",
    "            b = stack.pop() if stack else set()\n",
    "            a = stack.pop() if stack else set()\n",
    "            result = a & b\n",
    "            steps.append(f\"AND -> {a} ∩ {b}\\n= {result}\")\n",
    "            stack.append(result)\n",
    "        elif t == 'OR':\n",
    "            b = stack.pop() if stack else set()\n",
    "            a = stack.pop() if stack else set()\n",
    "            result = a | b\n",
    "            steps.append(f\"OR  -> {a} ∪ {b}\\n= {result}\")\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            docs = postings_set(t)\n",
    "            steps.append(f\"Terme '{t}' -> docs {docs}\")\n",
    "            stack.append(docs)\n",
    "\n",
    "    print('\\n[Classic] Détail des étapes:')\n",
    "    for i, s in enumerate(steps, 1):\n",
    "        print(f\"- Étape {i}: {s}\")\n",
    "    result_docs = sorted(list(stack.pop() if stack else set()))\n",
    "    print('\\n[Classic] Résultat final (docs):')\n",
    "    print(result_docs)\n",
    "    return result_docs\n",
    "\n",
    "def run_fuzzy_verbose(query_text, topk=20):\n",
    "    toks = preprocess_query(query_text)\n",
    "    print(\"\\n[Fuzzy] Requête :\", query_text)\n",
    "    print(\"[Fuzzy] Tokens  :\", toks)\n",
    "    rpn = to_rpn(toks)\n",
    "    print(\"[Fuzzy] RPN     :\", rpn)\n",
    "\n",
    "    stack = []\n",
    "    steps = []\n",
    "    for t in rpn:\n",
    "        if t == 'NOT':\n",
    "            a = stack.pop() if stack else {}\n",
    "            result = {d: 1.0 - a.get(d, 0.0) for d in all_docs}\n",
    "            steps.append(\"NOT -> 1 - score\")\n",
    "            stack.append(result)\n",
    "        elif t == 'AND':\n",
    "            b = stack.pop() if stack else {}\n",
    "            a = stack.pop() if stack else {}\n",
    "            result = {d: min(a.get(d, 0.0), b.get(d, 0.0)) for d in all_docs}\n",
    "            steps.append(\"AND -> min(scores)\")\n",
    "            stack.append(result)\n",
    "        elif t == 'OR':\n",
    "            b = stack.pop() if stack else {}\n",
    "            a = stack.pop() if stack else {}\n",
    "            result = {d: max(a.get(d, 0.0), b.get(d, 0.0)) for d in all_docs}\n",
    "            steps.append(\"OR  -> max(scores)\")\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            vec = term_score_vector(t)\n",
    "            steps.append(f\"Terme '{t}' -> RSV(d, t) (normalisé), {len(vec)} docs\")\n",
    "            stack.append(vec)\n",
    "\n",
    "    print('\\n[Fuzzy] Détail des étapes:')\n",
    "    for i, s in enumerate(steps, 1):\n",
    "        print(f\"- Étape {i}: {s}\")\n",
    "\n",
    "    scores = stack.pop() if stack else {}\n",
    "    ranking = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\n[Fuzzy] Top documents (doc, RSV):\")\n",
    "    for d, s in ranking[:topk]:\n",
    "        print(f\"{d}\\t{s:.4f}\")\n",
    "    return ranking\n",
    "\n",
    "\n",
    "query = \"(query AND reformulation) OR (language AND model)\"\n",
    "run_classic_verbose(query)\n",
    "run_fuzzy_verbose(query, topk=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6453765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: (10% AND 12%) OR (175 AND NOT 2)\n",
      "\n",
      "Doc\tScore\n",
      "D2\t0.1409\n",
      "D6\t0.0\n",
      "D4\t0.0\n",
      "D1\t0.0\n",
      "D5\t0.0\n",
      "D3\t0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD THE INVERTED INDEX FILE\n",
    "# Format: <Term> <Doc> <Freq> <TF-IDF>\n",
    "# ------------------------------------------------------------\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "\n",
    "# Read tab-separated txt file \n",
    "data = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "\n",
    "# Build a document-term matrix {doc: {term: tfidf}}\n",
    "doc_dict = defaultdict(dict)\n",
    "for _, row in data.iterrows():\n",
    "    doc_dict[row[\"doc\"]][str(row[\"term\"]).lower()] = float(row[\"tfidf\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. FUZZY BOOLEAN EVALUATION FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def fuzzy_and(a, b): return min(a, b)\n",
    "def fuzzy_or(a, b): return max(a, b)\n",
    "def fuzzy_not(a): return 1 - a\n",
    "\n",
    "def get_weight(doc, term):\n",
    "    \"\"\"Return TF-IDF weight of term in doc, or 0 if missing.\"\"\"\n",
    "    return doc_dict[doc].get(term.lower(), 0.0)\n",
    "\n",
    "def eval_fuzzy(query, doc):\n",
    "    \"\"\"Evaluate fuzzy boolean query for one document.\"\"\"\n",
    "    # Add spaces around parentheses\n",
    "    query = query.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    tokens = query.split()\n",
    "    \n",
    "    def parse(tokens):\n",
    "        stack = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tok = tokens[i].lower()\n",
    "            if tok == \"(\":\n",
    "                # find matching parenthesis\n",
    "                depth = 0\n",
    "                j = i\n",
    "                while j < len(tokens):\n",
    "                    if tokens[j] == \"(\":\n",
    "                        depth += 1\n",
    "                    elif tokens[j] == \")\":\n",
    "                        depth -= 1\n",
    "                        if depth == 0:\n",
    "                            break\n",
    "                    j += 1\n",
    "                val = parse(tokens[i + 1:j])\n",
    "                stack.append(val)\n",
    "                i = j\n",
    "            elif tok == \"and\":\n",
    "                stack.append(\"AND\")\n",
    "            elif tok == \"or\":\n",
    "                stack.append(\"OR\")\n",
    "            elif tok == \"not\":\n",
    "                # next token is negated\n",
    "                next_term = tokens[i + 1].lower()\n",
    "                w = fuzzy_not(get_weight(doc, next_term))\n",
    "                stack.append(w)\n",
    "                i += 1\n",
    "            else:\n",
    "                stack.append(get_weight(doc, tok))\n",
    "            i += 1\n",
    "        \n",
    "        # Evaluate AND first, then OR\n",
    "        while \"AND\" in stack:\n",
    "            idx = stack.index(\"AND\")\n",
    "            res = fuzzy_and(stack[idx - 1], stack[idx + 1])\n",
    "            stack = stack[:idx - 1] + [res] + stack[idx + 2:]\n",
    "        while \"OR\" in stack:\n",
    "            idx = stack.index(\"OR\")\n",
    "            res = fuzzy_or(stack[idx - 1], stack[idx + 1])\n",
    "            stack = stack[:idx - 1] + [res] + stack[idx + 2:]\n",
    "        return stack[0]\n",
    "\n",
    "    return parse(tokens)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. RUN A QUERY ON ALL DOCUMENTS\n",
    "# ------------------------------------------------------------\n",
    "def fuzzy_search(query):\n",
    "    results = {}\n",
    "    for doc in doc_dict.keys():\n",
    "        score = eval_fuzzy(query, doc)\n",
    "        results[doc] = round(score, 4)\n",
    "    # Sort descending by score\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST\n",
    "# ------------------------------------------------------------\n",
    "query = \"(10% AND 12%) OR (175 AND NOT 2)\"\n",
    "results = fuzzy_search(query)\n",
    "\n",
    "print(f\"\\n🔍 Query: {query}\\n\")\n",
    "print(\"Doc\\tScore\")\n",
    "for doc, score in results.items():\n",
    "    print(f\"{doc}\\t{score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd3da3",
   "metadata": {},
   "source": [
    "# Extended Boolean Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d99857",
   "metadata": {},
   "source": [
    "## 🔹 1. Concept Recap\n",
    "\n",
    "The **Extended Boolean Model** replaces the strict Boolean **AND / OR** logic with continuous functions that measure **how much a document satisfies a query**.\n",
    "\n",
    "It uses the **p-norm operator**, where the parameter \\( p \\) controls how strict or relaxed the matching is:\n",
    "\n",
    "| p value | Behavior |\n",
    "|:--------:|:----------|\n",
    "| \\( p \\to \\infty \\) | strict Boolean (perfect AND/OR) |\n",
    "| \\( p = 1 \\) | loose, soft matching (closer to vector model) |\n",
    "| **Typical value** | between 2 and 5 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Formulas\n",
    "\n",
    "Let \\( w_{d_i} \\) be the **TF-IDF weight** of term *i* in document *d*, normalized to [0, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 AND Query\n",
    "\n",
    "\\[\n",
    "S_{AND}(d, q) = \\left( \\frac{1}{n} \\sum_{i=1}^{n} (w_{d_i})^p \\right)^{\\frac{1}{p}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 OR Query\n",
    "\n",
    "\\[\n",
    "S_{OR}(d, q) = 1 - \\left( \\frac{1}{n} \\sum_{i=1}^{n} (1 - w_{d_i})^p \\right)^{\\frac{1}{p}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "where  \n",
    "\\( n \\) = number of terms in the query,  \n",
    "and \\( p \\) = the **p-norm parameter** controlling the **strictness of matching**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Intuition:**\n",
    "- When \\( p \\) is large → behavior approaches strict Boolean logic.  \n",
    "- When \\( p \\) is small → more flexible, similar to vector-space similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd59b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Extended Boolean Query: 10% AND 12% (p=2)\n",
      "\n",
      "Doc\tScore\n",
      "D2\t0.1992\n",
      "D4\t0.0664\n",
      "D6\t0.0\n",
      "D1\t0.0\n",
      "D5\t0.0\n",
      "D3\t0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD THE INVERTED INDEX\n",
    "# ------------------------------------------------------------\n",
    "inverted_path = \"results/inverted_index_weighted.txt\"\n",
    "\n",
    "data = pd.read_csv(inverted_path, sep=\"\\t\", header=None, names=[\"term\", \"doc\", \"freq\", \"tfidf\"])\n",
    "\n",
    "# Build structure: {doc: {term: tfidf}}\n",
    "doc_dict = defaultdict(dict)\n",
    "for _, row in data.iterrows():\n",
    "    doc_dict[row[\"doc\"]][str(row[\"term\"]).lower()] = float(row[\"tfidf\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. EXTENDED BOOLEAN MODEL FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def extended_boolean_score(doc_weights, query_terms, operator=\"AND\", p=2):\n",
    "    \"\"\"\n",
    "    Compute the Extended Boolean model score for one document.\n",
    "    - doc_weights: dict {term: weight}\n",
    "    - query_terms: list of query tokens (strings)\n",
    "    - operator: 'AND' or 'OR'\n",
    "    - p: float, p-norm parameter\n",
    "    \"\"\"\n",
    "    # Extract weights for query terms (default 0 if term not in doc)\n",
    "    w = np.array([doc_weights.get(term.lower(), 0.0) for term in query_terms])\n",
    "    n = len(w)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if operator.upper() == \"AND\":\n",
    "        return (np.sum(w ** p) / n) ** (1 / p)\n",
    "    elif operator.upper() == \"OR\":\n",
    "        return 1 - ((np.sum((1 - w) ** p) / n) ** (1 / p))\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be 'AND' or 'OR'\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. QUERY EXECUTION\n",
    "# ------------------------------------------------------------\n",
    "def extended_boolean_search(query, p=2):\n",
    "    \"\"\"\n",
    "    Execute an AND/OR query across all documents using the Extended Boolean Model.\n",
    "    \"\"\"\n",
    "    # Detect operator\n",
    "    query_lower = query.lower()\n",
    "    if \" and \" in query_lower:\n",
    "        operator = \"AND\"\n",
    "        terms = [t.strip() for t in query_lower.split(\"and\")]\n",
    "    elif \" or \" in query_lower:\n",
    "        operator = \"OR\"\n",
    "        terms = [t.strip() for t in query_lower.split(\"or\")]\n",
    "    else:\n",
    "        operator = \"AND\"\n",
    "        terms = [query_lower.strip()]\n",
    "    \n",
    "    # Compute scores for all docs\n",
    "    results = {}\n",
    "    for doc, weights in doc_dict.items():\n",
    "        score = extended_boolean_score(weights, terms, operator=operator, p=p)\n",
    "        results[doc] = round(score, 4)\n",
    "\n",
    "    # Sort results by descending score\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST\n",
    "# ------------------------------------------------------------\n",
    "query = \"10% AND 12%\"\n",
    "p = 2  # try 1, 2, or higher for stricter matching\n",
    "\n",
    "results = extended_boolean_search(query, p=p)\n",
    "\n",
    "print(f\"\\n🔍 Extended Boolean Query: {query} (p={p})\\n\")\n",
    "print(\"Doc\\tScore\")\n",
    "for doc, score in results.items():\n",
    "    print(f\"{doc}\\t{score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
