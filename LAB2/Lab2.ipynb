{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e0180c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, collections\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "# Point these to your Lab1 outputs (Porter, weighted)\n",
    "OUTPUT_DIR = \"results\"\n",
    "INVERTED_WEIGHTED = os.path.join(OUTPUT_DIR, \"inverted_index_weighted.txt\")\n",
    "# file format per Lab1: \"<Term>\\tDocID\\t<TF_max>\\t<Weight>\" or space-separated\n",
    "\n",
    "# Query to test (Task 1)\n",
    "RAW_QUERY = \"(query AND reformulation) OR (Language AND model)\"\n",
    "\n",
    "# ---------- PREPROC (reuse Lab1 pipeline) ----------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Tokenizer that preserves words and simple tokens; enough for queries\n",
    "TOKENIZER = RegexpTokenizer(\n",
    "    r'(?:[A-Za-z]\\.){2,}'     # e.g., \"e.g.\"\n",
    "    r'|[A-Za-z]+(?:-[A-Za-z]+)+'  # hyphenated words\n",
    "    r'|[A-Za-z]+'             # words\n",
    ")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # only drop stopwords for pure alpha tokens\n",
    "    return [t for t in tokens if not (t.isalpha() and t in stop_words)]\n",
    "\n",
    "def stem_porter(tokens):\n",
    "    return [porter.stem(t) for t in tokens]\n",
    "\n",
    "def preprocess_text_to_terms(text):\n",
    "    toks = TOKENIZER.tokenize(text.lower())\n",
    "    toks = remove_stopwords(toks)\n",
    "    stems = stem_porter(toks)\n",
    "    return stems\n",
    "\n",
    "# ---------- LOAD INVERTED INDEX (from Lab1) ----------\n",
    "# Build postings: term -> {docId(str like \"D1\"): degree in [0,1]}\n",
    "# We'll take TF_max (already in [0,1]) as the membership degree μ_t(d)\n",
    "postings = collections.defaultdict(dict)\n",
    "all_docs = set()\n",
    "\n",
    "def _split_line(line):\n",
    "    # Accept both tab or space separated\n",
    "    if \"\\t\" in line:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "    else:\n",
    "        parts = line.strip().split()\n",
    "    return parts\n",
    "\n",
    "with open(INVERTED_WEIGHTED, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): \n",
    "            continue\n",
    "        parts = _split_line(line)\n",
    "        # Expect 4 columns: term, doc, tf_norm, weight\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        term, doc, tf_norm_str, _ = parts[0], parts[1], parts[2], parts[3]\n",
    "        # Normalize doc like \"D3\" → \"D3\"\n",
    "        doc = doc.strip()\n",
    "        try:\n",
    "            tf_norm = float(tf_norm_str)\n",
    "        except:\n",
    "            continue\n",
    "        postings[term][doc] = tf_norm\n",
    "        all_docs.add(doc)\n",
    "\n",
    "all_docs = sorted(all_docs, key=lambda d: int(d[1:]))  # [\"D1\",\"D2\",...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "654ac29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- BOOLEAN PARSER ----------\n",
    "# We’ll parse infix boolean expressions with precedence: NOT > AND > OR\n",
    "# Supported tokens: terms, AND, OR, NOT, parentheses\n",
    "OPERATORS = {\"NOT\": 3, \"AND\": 2, \"OR\": 1}  # precedence\n",
    "\n",
    "def tokenize_boolean_query(q):\n",
    "    # split on spaces and parentheses, keep parentheses\n",
    "    q = q.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    raw = q.strip().split()\n",
    "    return raw\n",
    "\n",
    "def preprocess_boolean_terms(tokens):\n",
    "    # Lower + stem non-operator tokens\n",
    "    result = []\n",
    "    for t in tokens:\n",
    "        up = t.upper()\n",
    "        if up in (\"AND\",\"OR\",\"NOT\",\"(\",\")\"):\n",
    "            result.append(up)\n",
    "        else:\n",
    "            # preprocess as a single-term text\n",
    "            stemmed = preprocess_text_to_terms(t)\n",
    "            if len(stemmed) == 0:\n",
    "                continue\n",
    "            # queries like \"Language\" -> [\"languag\"]\n",
    "            result.append(stemmed[0])\n",
    "    return result\n",
    "\n",
    "def infix_to_rpn(tokens):\n",
    "    # Shunting-yard to RPN\n",
    "    out = []\n",
    "    stack = []\n",
    "    for tok in tokens:\n",
    "        if tok in (\"AND\",\"OR\",\"NOT\"):\n",
    "            while stack and stack[-1] in OPERATORS and OPERATORS[stack[-1]] >= OPERATORS[tok]:\n",
    "                out.append(stack.pop())\n",
    "            stack.append(tok)\n",
    "        elif tok == \"(\":\n",
    "            stack.append(tok)\n",
    "        elif tok == \")\":\n",
    "            while stack and stack[-1] != \"(\":\n",
    "                out.append(stack.pop())\n",
    "            if stack and stack[-1] == \"(\":\n",
    "                stack.pop()\n",
    "        else:\n",
    "            # term\n",
    "            out.append(tok)\n",
    "    while stack:\n",
    "        out.append(stack.pop())\n",
    "    return out\n",
    "\n",
    "# ---------- CLASSIC BOOLEAN ----------\n",
    "def eval_classic_boolean(rpn):\n",
    "    # Evaluate to a SET of docs\n",
    "    stack = []\n",
    "    for tok in rpn:\n",
    "        if tok == \"NOT\":\n",
    "            a = stack.pop()\n",
    "            res = set(all_docs) - a\n",
    "            stack.append(res)\n",
    "        elif tok == \"AND\":\n",
    "            b = stack.pop(); a = stack.pop()\n",
    "            stack.append(a & b)\n",
    "        elif tok == \"OR\":\n",
    "            b = stack.pop(); a = stack.pop()\n",
    "            stack.append(a | b)\n",
    "        else:\n",
    "            # term -> set of docs that contain the term (non-zero degree)\n",
    "            docs = set(postings.get(tok, {}).keys())\n",
    "            stack.append(docs)\n",
    "    return stack.pop() if stack else set()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c1495a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- FUZZY BOOLEAN (min/max, NOT->1-x) ----------\n",
    "def fuzzy_not(x): return 1.0 - x\n",
    "def fuzzy_and(a, b): return min(a, b)\n",
    "def fuzzy_or(a, b):  return max(a, b)\n",
    "\n",
    "def eval_fuzzy_boolean(rpn):\n",
    "    # Evaluate to a dict: doc -> degree in [0,1]\n",
    "    stack = []\n",
    "    for tok in rpn:\n",
    "        if tok == \"NOT\":\n",
    "            A = stack.pop()\n",
    "            stack.append({d: fuzzy_not(A.get(d, 0.0)) for d in all_docs})\n",
    "        elif tok in (\"AND\",\"OR\"):\n",
    "            B = stack.pop(); A = stack.pop()\n",
    "            if tok == \"AND\":\n",
    "                stack.append({d: fuzzy_and(A.get(d,0.0), B.get(d,0.0)) for d in all_docs})\n",
    "            else:\n",
    "                stack.append({d: fuzzy_or(A.get(d,0.0), B.get(d,0.0)) for d in all_docs})\n",
    "        else:\n",
    "            # term -> membership function μ_t(d) = tf_max(d,t)\n",
    "            μ = {d: postings.get(tok, {}).get(d, 0.0) for d in all_docs}\n",
    "            stack.append(μ)\n",
    "        # Normalize each term's doc frequencies to [0,1]\n",
    "        # for term, docs in postings.items():\n",
    "        #     max_tf = max(docs.values()) if docs else 1.0\n",
    "        #     postings[term] = {d: tf / max_tf for d, tf in docs.items()}\n",
    "\n",
    "    return stack.pop() if stack else {d:0.0 for d in all_docs}\n",
    "\n",
    "# ---------- EXTENDED BOOLEAN (p-norm; Salton) ----------\n",
    "# For two inputs a,b in [0,1]:\n",
    "# OR_p(a,b)  = ((a^p + b^p)/2)^(1/p)\n",
    "# AND_p(a,b) = 1 - (((1-a)^p + (1-b)^p)/2)^(1/p)\n",
    "def or_p(a, b, p):  return ((a**p + b**p)/2.0)**(1.0/p)\n",
    "def and_p(a, b, p): return 1.0 - (((1.0-a)**p + (1.0-b)**p)/2.0)**(1.0/p)\n",
    "\n",
    "def eval_extended_boolean(rpn, p=2.0):\n",
    "    stack = []\n",
    "    for tok in rpn:\n",
    "        if tok == \"NOT\":\n",
    "            A = stack.pop()\n",
    "            stack.append({d: 1.0 - A.get(d,0.0) for d in all_docs})\n",
    "        elif tok in (\"AND\",\"OR\"):\n",
    "            B = stack.pop(); A = stack.pop()\n",
    "            if tok == \"AND\":\n",
    "                stack.append({d: and_p(A.get(d,0.0), B.get(d,0.0), p) for d in all_docs})\n",
    "            else:\n",
    "                stack.append({d: or_p(A.get(d,0.0),  B.get(d,0.0), p) for d in all_docs})\n",
    "        else:\n",
    "            μ = {d: postings.get(tok, {}).get(d, 0.0) for d in all_docs}\n",
    "            stack.append(μ)\n",
    "\n",
    "    return stack.pop() if stack else {d:0.0 for d in all_docs}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de7d9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- RUN TASK 1 ----------\n",
    "def run_task1(raw_query=RAW_QUERY, p=2.0, topk=10):\n",
    "    # 1) preprocess query like docs (per lab)\n",
    "    toks = tokenize_boolean_query(raw_query)\n",
    "    toks = preprocess_boolean_terms(toks)\n",
    "    rpn  = infix_to_rpn(toks)\n",
    "\n",
    "    # 2) Classic Boolean\n",
    "    classic = eval_classic_boolean(rpn)\n",
    "\n",
    "    # 3) Fuzzy Boolean (rank)\n",
    "    fuzzy_scores = eval_fuzzy_boolean(rpn)\n",
    "    fuzzy_rank = sorted(((d, fuzzy_scores[d]) for d in all_docs), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 4) Extended Boolean (rank), p=2 by default\n",
    "    ext_scores = eval_extended_boolean(rpn, p=p)\n",
    "    ext_rank = sorted(((d, ext_scores[d]) for d in all_docs), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # pretty print\n",
    "    print(\"Query (raw):\", raw_query)\n",
    "    print(\"Query (stemmed):\", toks)\n",
    "    print(\"\\n=== Classic Boolean (exact match) ===\")\n",
    "    print(\"Matches:\", \" \".join(sorted(classic, key=lambda d: int(d[1:]))) if classic else \"None\")\n",
    "\n",
    "    print(\"\\n=== Fuzzy Boolean (min/max) — top docs ===\")\n",
    "    for d, s in fuzzy_rank[:topk]:\n",
    "        print(f\"{d}\\t{s:.3f}\")\n",
    "\n",
    "    print(\"\\n=== Extended Boolean (p-norm, p={p}) — top docs ===\".format(p=p))\n",
    "    for d, s in ext_rank[:topk]:\n",
    "        print(f\"{d}\\t{s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19a7b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (raw): (query AND reformulation) OR (Language AND model)\n",
      "Query (stemmed): ['(', 'queri', 'AND', 'reformul', ')', 'OR', '(', 'languag', 'AND', 'model', ')']\n",
      "\n",
      "=== Classic Boolean (exact match) ===\n",
      "Matches: D1 D2 D3 D4 D5 D6\n",
      "\n",
      "=== Fuzzy Boolean (min/max) — top docs ===\n",
      "D4\t4.000\n",
      "D1\t3.000\n",
      "D5\t2.000\n",
      "D2\t1.000\n",
      "D3\t1.000\n",
      "D6\t1.000\n",
      "\n",
      "=== Extended Boolean (p-norm, p=2.0) — top docs ===\n",
      "D4\t3.571\n",
      "D3\t2.793\n",
      "D1\t1.000\n",
      "D5\t0.460\n",
      "D2\t0.359\n",
      "D6\t0.207\n"
     ]
    }
   ],
   "source": [
    "run_task1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
