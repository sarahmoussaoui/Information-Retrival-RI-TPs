{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d593d0d7",
   "metadata": {},
   "source": [
    "Key Formulas Implemented:\n",
    "\n",
    "Without Learning:\n",
    "\n",
    "RSV_d = Σ [log( (N - n_i + 0.5) / (n_i + 0.5) )]\n",
    "\n",
    "With Learning:\n",
    "\n",
    "RSV_d = Σ [log( ((r_i + 0.5)/(R - r_i + 0.5)) / ((n_i - r_i + 0.5)/(N - n_i - R + r_i + 0.5)) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4c0ff",
   "metadata": {},
   "source": [
    "# Classic BIR – Without and without Learning Data\n",
    "- Build the classic Binary Independence Retrieval (BIR) model using the smoothed\n",
    "formula without learning data (i.e. no relevance feedback). For each query, compute the\n",
    "RSV score for every document and return a ranking (best first).\n",
    "Hint: Build the binary term–document matrix first\n",
    "\n",
    "- Build the classic Binary Independence Retrieval (BIR) model using the smoothed\n",
    "formula with learning data (given in the table below). For each query, compute the RSV\n",
    "score for every document and return a ranking (best first) and give a short interpretation\n",
    "of the result\n",
    "\n",
    "Query | Relevant documents for the query\n",
    "q1  D2, D4\n",
    "q2 D2, D4\n",
    "q3 D4, D1\n",
    "q4 D2, D1\n",
    "q5 D3, D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2367aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "Query: 'large language models for information retrieval and ranking'\n",
      "\n",
      " BIR (no learning) ranking:\n",
      "   D1     score=-6.519275\n",
      "   D2     score=-7.107061\n",
      "   D4     score=-7.694848\n",
      "   D3     score=-8.282635\n",
      "   D5     score=-8.282635\n",
      "   D6     score=-8.282635\n",
      "\n",
      " BIR (with learning) ranking (using relevance set):\n",
      "   D1     score=-0.068764\n",
      "   D2     score=-0.916062\n",
      "   D4     score=-1.763360\n",
      "   D3     score=-2.610658\n",
      "   D5     score=-2.610658\n",
      "   D6     score=-2.610658\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "Query: 'LLM for information retrieval and Ranking'\n",
      "\n",
      " BIR (no learning) ranking:\n",
      "   D1     score=1.175573\n",
      "   D2     score=-0.711496\n",
      "   D4     score=-1.299283\n",
      "   D3     score=-1.887070\n",
      "   D5     score=-1.887070\n",
      "   D6     score=-1.887070\n",
      "\n",
      " BIR (with learning) ranking (using relevance set):\n",
      "   D1     score=1.694596\n",
      "   D2     score=1.609438\n",
      "   D4     score=0.762140\n",
      "   D3     score=-0.085158\n",
      "   D5     score=-0.085158\n",
      "   D6     score=-0.085158\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "Query: 'query Reformulation in information retrieval'\n",
      "\n",
      " BIR (no learning) ranking:\n",
      "   D1     score=0.587787\n",
      "   D4     score=0.000000\n",
      "   D2     score=-0.587787\n",
      "   D3     score=-0.587787\n",
      "   D6     score=-0.587787\n",
      "   D5     score=-1.175573\n",
      "\n",
      " BIR (with learning) ranking (using relevance set):\n",
      "   D1     score=9.222763\n",
      "   D4     score=8.375465\n",
      "   D2     score=1.609438\n",
      "   D5     score=0.762140\n",
      "   D3     score=-0.847298\n",
      "   D6     score=-0.847298\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "Query: 'ranking Documents'\n",
      "\n",
      " BIR (no learning) ranking:\n",
      "   D1     score=0.587787\n",
      "   D2     score=0.587787\n",
      "   D3     score=0.000000\n",
      "   D4     score=0.000000\n",
      "   D5     score=0.000000\n",
      "   D6     score=0.000000\n",
      "\n",
      " BIR (with learning) ranking (using relevance set):\n",
      "   D1     score=6.263398\n",
      "   D2     score=6.263398\n",
      "   D5     score=2.456736\n",
      "   D3     score=0.000000\n",
      "   D4     score=0.000000\n",
      "   D6     score=0.000000\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "Query: 'Optimizing recommendation systems with LLMs by leveraging item metadata'\n",
      "\n",
      " BIR (no learning) ranking:\n",
      "   D3     score=0.464077\n",
      "   D6     score=0.464077\n",
      "   D1     score=0.000000\n",
      "   D4     score=0.000000\n",
      "   D2     score=-1.299283\n",
      "   D5     score=-1.299283\n",
      "\n",
      " BIR (with learning) ranking (using relevance set):\n",
      "   D3     score=12.182128\n",
      "   D6     score=12.182128\n",
      "   D2     score=0.762140\n",
      "   D5     score=-1.694596\n",
      "   D1     score=-2.456736\n",
      "   D4     score=-2.456736\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 1️⃣ Prétraitement: Tokenisation + Stemming Porter\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"\n",
    "    Tokenisation simple + stemming Porter\n",
    "    \"\"\"\n",
    "    tokens = text.lower().replace(',', ' ').replace('.', ' ').split()\n",
    "    tokens = [ps.stem(t) for t in tokens if len(t) > 1]\n",
    "    return tokens\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2️⃣ Construction de la matrice terme-document binaire\n",
    "# -------------------------------------------------------------\n",
    "class BIRModel:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = []\n",
    "        self.doc_ids = []\n",
    "        self.W = None  # matrice binaire terme-document\n",
    "        self.N = 0     # nombre de documents\n",
    "        self.term_doc_count = None  # ni = nombre de documents contenant le terme\n",
    "        \n",
    "    def load_term_doc_file(self, filepath):\n",
    "        \n",
    "        term_docs = defaultdict(set)\n",
    "        all_docs = set()\n",
    "        all_terms = set()\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    term = parts[0]\n",
    "                    doc_id = parts[1]\n",
    "                    all_terms.add(term)\n",
    "                    all_docs.add(doc_id)\n",
    "                    term_docs[term].add(doc_id)\n",
    "        \n",
    "        self.vocabulary = sorted(list(all_terms))\n",
    "        self.doc_ids = sorted(list(all_docs))\n",
    "        self.N = len(self.doc_ids)\n",
    "        M = len(self.vocabulary)\n",
    "        \n",
    "        self.W = np.zeros((M, self.N), dtype=int)\n",
    "        self.term_doc_count = np.zeros(M, dtype=int)\n",
    "        \n",
    "        for i, term in enumerate(self.vocabulary):\n",
    "            docs = term_docs[term]\n",
    "            for j, doc_id in enumerate(self.doc_ids):\n",
    "                if doc_id in docs:\n",
    "                    self.W[i,j] = 1\n",
    "            self.term_doc_count[i] = len(docs)\n",
    "        \n",
    "        # print(f\"Loaded: {M} terms, {self.N} documents (N={self.N})\")\n",
    "        # print(f\"Sparsity: {100*(1-np.count_nonzero(self.W)/(M*self.N)):.2f}%\")\n",
    "    \n",
    "    # 3️⃣ Calcul RSV sans apprentissage (smoothed)\n",
    "    def compute_rsv_no_learning(self, query_tokens):\n",
    "        \n",
    "        scores = np.zeros(self.N)\n",
    "        found_terms = []\n",
    "        for t in query_tokens:\n",
    "            if t in self.vocabulary:\n",
    "                i = self.vocabulary.index(t)\n",
    "                ni = self.term_doc_count[i]\n",
    "                weight = math.log((self.N - ni + 0.5)/(ni + 0.5))\n",
    "                scores += weight * self.W[i,:]\n",
    "                found_terms.append(t)\n",
    "        return scores, found_terms\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 4️⃣ Calcul RSV avec apprentissage (relevance feedback)\n",
    "    # -------------------------------------------------------------\n",
    "    def compute_rsv_with_learning(self, query_tokens, relevant_docs):\n",
    "       \n",
    "        scores = np.zeros(self.N)\n",
    "        R = len(relevant_docs)\n",
    "        rel_indices = [self.doc_ids.index(d) for d in relevant_docs]\n",
    "        found_terms = []\n",
    "        \n",
    "        for t in query_tokens:\n",
    "            if t in self.vocabulary:\n",
    "                i = self.vocabulary.index(t)\n",
    "                ni = self.term_doc_count[i]\n",
    "                ri = sum(self.W[i, idx] for idx in rel_indices)  # occurrences dans docs pertinents\n",
    "                # smoothed formula\n",
    "                weight = math.log( ((ri+0.5)/(R-ri+0.5)) / ((ni-ri+0.5)/(self.N - ni - R + ri + 0.5)) )\n",
    "                scores += weight * self.W[i,:]\n",
    "                found_terms.append(t)\n",
    "        return scores, found_terms\n",
    "    \n",
    "    \n",
    "    # 5️⃣ Classement et affichage\n",
    "    def rank_query(self, query_text, relevant_docs=None):\n",
    "        print(\"*\"*70)\n",
    "        print(f\"Query: '{query_text}'\")\n",
    "        \n",
    "        query_tokens = tokenize_and_stem(query_text)\n",
    "        # print(f\" tokens: {query_tokens}\")\n",
    "        \n",
    "        # BIR sans apprentissage\n",
    "        scores_no, found_no = self.compute_rsv_no_learning(query_tokens)\n",
    "        # print(f\" tokens found in vocab: {found_no}\")\n",
    "        ranking_no = [(self.doc_ids[i], scores_no[i]) for i in range(self.N)]\n",
    "        ranking_no.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(\"\\n BIR (no learning) ranking:\")\n",
    "        for doc, score in ranking_no:\n",
    "            print(f\"   {doc:<6} score={score:.6f}\")\n",
    "        \n",
    "        # BIR avec apprentissage\n",
    "        if relevant_docs:\n",
    "            scores_learn, found_learn = self.compute_rsv_with_learning(query_tokens, relevant_docs)\n",
    "            ranking_learn = [(self.doc_ids[i], scores_learn[i]) for i in range(self.N)]\n",
    "            ranking_learn.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\"\\n BIR (with learning) ranking (using relevance set):\")\n",
    "            for doc, score in ranking_learn:\n",
    "                print(f\"   {doc:<6} score={score:.6f}\")\n",
    "\n",
    "# 6️⃣ Main\n",
    "def main():\n",
    "    tfidf_file = \"results/inverted_index_weighted.txt\"\n",
    "    \n",
    "    if not os.path.exists(tfidf_file):\n",
    "        print(f\"ERROR: file not found: {tfidf_file}\")\n",
    "        return\n",
    "    \n",
    "    bir = BIRModel()\n",
    "    bir.load_term_doc_file(tfidf_file)\n",
    "    \n",
    "    # Requêtes\n",
    "    queries = [\n",
    "        \"large language models for information retrieval and ranking\",\n",
    "        \"LLM for information retrieval and Ranking\",\n",
    "        \"query Reformulation in information retrieval\",\n",
    "        \"ranking Documents\",\n",
    "        \"Optimizing recommendation systems with LLMs by leveraging item metadata\"\n",
    "    ]\n",
    "    \n",
    "    # Relevance sets pour l'apprentissage\n",
    "    relevance_sets = {\n",
    "        'q1': ['D2', 'D4'],\n",
    "        'q2': ['D2', 'D4'],\n",
    "        'q3': ['D4', 'D1'],\n",
    "        'q4': ['D2', 'D1'],\n",
    "        'q5': ['D3', 'D6']\n",
    "    }\n",
    "    \n",
    "    for i, qtext in enumerate(queries, 1):\n",
    "        qid = f'q{i}'\n",
    "        relevant_docs = relevance_sets.get(qid, None)\n",
    "        bir.rank_query(qtext, relevant_docs=relevant_docs)\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75fac06",
   "metadata": {},
   "source": [
    "# Extended BIR with and without learning data\n",
    "Implement the Extended Binary Independence Retrieval (Extended BIR) model, which\n",
    "generalizes the Classic BIR by incorporating term frequency information rather than\n",
    "treating documents as purely binary.\n",
    "You will:\n",
    "- Use TF–IDF values as document term weights,\n",
    "- Use Binary query vectors (1 if term appears in query, 0 otherwise),\n",
    "- Use Same probabilistic weighting formulas given in lecture notes\n",
    "- Implement 2 models : Extended BIR With and without learning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a67395",
   "metadata": {},
   "source": [
    "Key Formulas Implemented:\n",
    "\n",
    "Without Learning:\n",
    "\n",
    "RSV_d = Σ [w_dt × qtfi × log( (N - n_i + 0.5) / (n_i + 0.5) )]\n",
    "\n",
    "With Learning:\n",
    "\n",
    "RSV_d = Σ [w_dt × qtfi × log( ((r_i + 0.5)/(R - r_i + 0.5)) / ((n_i - r_i + 0.5)/(N - n_i - R + r_i + 0.5)) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class ExtendedBIR:\n",
    "    def __init__(self, inverted_index_file):\n",
    "        self.term_doc_matrix, self.doc_terms, self.doc_lengths, self.term_doc_freq = self.parse_weighted_index(inverted_index_file)\n",
    "        self.all_docs = list(self.doc_terms.keys())\n",
    "        self.all_terms = list(self.term_doc_matrix.keys())\n",
    "        self.N = len(self.all_docs)\n",
    "        \n",
    "        # Precompute TF-IDF weights\n",
    "        self.tfidf_matrix = self.compute_tfidf_matrix()\n",
    "    \n",
    "    def parse_weighted_index(self, filename):\n",
    "        \"\"\"\n",
    "        Parse the weighted inverted index with format: term doc_id freq weight\n",
    "        \"\"\"\n",
    "        term_doc_matrix = defaultdict(dict)\n",
    "        doc_terms = defaultdict(set)\n",
    "        doc_lengths = defaultdict(int)\n",
    "        term_doc_freq = defaultdict(int)\n",
    "        term_weights = defaultdict(dict)\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 4:\n",
    "                    term = parts[0]\n",
    "                    doc_id = parts[1]\n",
    "                    freq = int(parts[2])\n",
    "                    weight = float(parts[3])\n",
    "                    \n",
    "                    term_doc_matrix[term][doc_id] = freq\n",
    "                    term_weights[term][doc_id] = weight\n",
    "                    doc_terms[doc_id].add(term)\n",
    "                    doc_lengths[doc_id] += 1\n",
    "                    term_doc_freq[term] += 1\n",
    "        \n",
    "        self.term_weights = term_weights\n",
    "        return term_doc_matrix, doc_terms, doc_lengths, term_doc_freq\n",
    "    \n",
    "    def compute_tfidf_matrix(self):\n",
    "        \"\"\"\n",
    "        Compute TF-IDF weights for all term-document pairs\n",
    "        \"\"\"\n",
    "        tfidf_matrix = defaultdict(dict)\n",
    "        \n",
    "        for term in self.all_terms:\n",
    "            idf = math.log(self.N / self.term_doc_freq[term]) if self.term_doc_freq[term] > 0 else 0\n",
    "            \n",
    "            for doc in self.all_docs:\n",
    "                if doc in self.term_doc_matrix[term]:\n",
    "                    # Use the provided weight as TF component, or frequency\n",
    "                    tf = self.term_weights[term].get(doc, self.term_doc_matrix[term][doc])\n",
    "                    tfidf = tf * idf\n",
    "                    tfidf_matrix[doc][term] = tfidf\n",
    "                else:\n",
    "                    tfidf_matrix[doc][term] = 0\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def get_document_vector(self, doc_id):\n",
    "        \"\"\"Get TF-IDF vector for a document\"\"\"\n",
    "        return self.tfidf_matrix[doc_id]\n",
    "    \n",
    "    def get_query_weights(self, query_terms):\n",
    "        \"\"\"\n",
    "        Get query term weights (qtfi)\n",
    "        For binary queries, qtfi = 1 if term appears in query\n",
    "        Can be extended for weighted queries\n",
    "        \"\"\"\n",
    "        query_weights = {}\n",
    "        for term in self.all_terms:\n",
    "            query_weights[term] = 1 if term in query_terms else 0\n",
    "        return query_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc36029",
   "metadata": {},
   "source": [
    "## without learning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedBIRWithoutLearning(ExtendedBIR):\n",
    "    def __init__(self, inverted_index_file):\n",
    "        super().__init__(inverted_index_file)\n",
    "    \n",
    "    def compute_rsv(self, query_terms):\n",
    "        \"\"\"\n",
    "        Compute RSV using Extended BIR formula without learning data:\n",
    "        RSV_d = sum_{t in q} [w_dt * qtfi * log( (N - n_i + 0.5) / (n_i + 0.5) )]\n",
    "        \n",
    "        Where:\n",
    "        - w_dt: TF-IDF weight of term t in document d\n",
    "        - qtfi: Query term weight (1 for binary queries)\n",
    "        - N: Total number of documents\n",
    "        - n_i: Number of documents containing term i\n",
    "        \"\"\"\n",
    "        rsv_scores = {}\n",
    "        query_weights = self.get_query_weights(query_terms)\n",
    "        \n",
    "        for doc in self.all_docs:\n",
    "            score = 0.0\n",
    "            doc_vector = self.get_document_vector(doc)\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.term_doc_matrix:\n",
    "                    # Get TF-IDF weight for the term in this document\n",
    "                    w_dt = doc_vector.get(term, 0)\n",
    "                    \n",
    "                    # Get query term weight\n",
    "                    qtfi = query_weights.get(term, 0)\n",
    "                    \n",
    "                    # Get document frequency\n",
    "                    n_i = self.term_doc_freq.get(term, 0)\n",
    "                    \n",
    "                    # Compute the log ratio with smoothing\n",
    "                    numerator = self.N - n_i + 0.5\n",
    "                    denominator = n_i + 0.5\n",
    "                    \n",
    "                    # Avoid division by zero and negative values\n",
    "                    if denominator <= 0:\n",
    "                        denominator = 0.5\n",
    "                    if numerator <= 0:\n",
    "                        numerator = 0.5\n",
    "                    \n",
    "                    log_ratio = math.log(numerator / denominator)\n",
    "                    \n",
    "                    # Extended BIR formula\n",
    "                    score += w_dt * qtfi * log_ratio\n",
    "            \n",
    "            rsv_scores[doc] = score\n",
    "        \n",
    "        return rsv_scores\n",
    "    \n",
    "    def rank_documents(self, query_terms):\n",
    "        \"\"\"Rank documents by RSV score\"\"\"\n",
    "        rsv_scores = self.compute_rsv(query_terms)\n",
    "        ranked_docs = sorted(rsv_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked_docs\n",
    "    \n",
    "    def display_scoring_breakdown(self, query_terms, top_k=3):\n",
    "        \"\"\"Display detailed scoring breakdown for interpretation\"\"\"\n",
    "        print(f\"\\nSCORING BREAKDOWN for query: {query_terms}\")\n",
    "        print(f\"{'Term':<10} {'n_i':<6} {'N-n_i':<8} {'log_ratio':<12} {'IDF-like':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in self.term_doc_matrix:\n",
    "                n_i = self.term_doc_freq.get(term, 0)\n",
    "                numerator = self.N - n_i + 0.5\n",
    "                denominator = n_i + 0.5\n",
    "                log_ratio = math.log(numerator / denominator)\n",
    "                idf = math.log(self.N / n_i) if n_i > 0 else 0\n",
    "                \n",
    "                print(f\"{term:<10} {n_i:<6} {self.N-n_i:<8} {log_ratio:<12.4f} {idf:<10.4f}\")\n",
    "        \n",
    "        # Show top documents and their term contributions\n",
    "        ranked_results = self.rank_documents(query_terms)[:top_k]\n",
    "        print(f\"\\nTOP {top_k} DOCUMENTS - TERM CONTRIBUTIONS:\")\n",
    "        print(f\"{'Doc':<6} {'Total RSV':<12} {'Term Contributions':<40}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for doc, total_score in ranked_results:\n",
    "            doc_vector = self.get_document_vector(doc)\n",
    "            contributions = []\n",
    "            for term in query_terms:\n",
    "                if term in self.term_doc_matrix:\n",
    "                    w_dt = doc_vector.get(term, 0)\n",
    "                    qtfi = 1\n",
    "                    n_i = self.term_doc_freq.get(term, 0)\n",
    "                    log_ratio = math.log((self.N - n_i + 0.5) / (n_i + 0.5))\n",
    "                    term_contrib = w_dt * qtfi * log_ratio\n",
    "                    contributions.append(f\"{term}:{term_contrib:.3f}\")\n",
    "            \n",
    "            print(f\"{doc:<6} {total_score:<12.4f} {', '.join(contributions):<40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8f8cd",
   "metadata": {},
   "source": [
    "## With learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6474be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedBIRWithLearning(ExtendedBIR):\n",
    "    def __init__(self, inverted_index_file, relevance_data):\n",
    "        super().__init__(inverted_index_file)\n",
    "        self.relevance_data = relevance_data\n",
    "    \n",
    "    def compute_rsv(self, query_id, query_terms):\n",
    "        \"\"\"\n",
    "        Compute RSV using Extended BIR formula with learning data:\n",
    "        RSV_d = sum_{t in q} [w_dt * qtfi * log( ((r_i + 0.5)/(R - r_i + 0.5)) / ((n_i - r_i + 0.5)/(N - n_i - R + r_i + 0.5)) )]\n",
    "        \n",
    "        Where:\n",
    "        - w_dt: TF-IDF weight of term t in document d\n",
    "        - qtfi: Query term weight (1 for binary queries)\n",
    "        - r_i: Number of relevant documents containing term i\n",
    "        - R: Total number of relevant documents for this query\n",
    "        - n_i: Number of documents containing term i\n",
    "        - N: Total number of documents\n",
    "        \"\"\"\n",
    "        if query_id not in self.relevance_data:\n",
    "            # Fallback to non-learning version if no relevance data\n",
    "            bir_without = ExtendedBIRWithoutLearning(\"results/inverted_index_weighted.txt\")\n",
    "            return bir_without.compute_rsv(query_terms)\n",
    "        \n",
    "        rel_docs = self.relevance_data[query_id]['relevant']\n",
    "        R = len(rel_docs)\n",
    "        \n",
    "        rsv_scores = {}\n",
    "        query_weights = self.get_query_weights(query_terms)\n",
    "        \n",
    "        # Precompute r_i for each term\n",
    "        r_i = {}\n",
    "        for term in query_terms:\n",
    "            if term in self.term_doc_matrix:\n",
    "                r_i[term] = sum(1 for doc in rel_docs if doc in self.term_doc_matrix[term])\n",
    "        \n",
    "        for doc in self.all_docs:\n",
    "            score = 0.0\n",
    "            doc_vector = self.get_document_vector(doc)\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.term_doc_matrix:\n",
    "                    # Get weights\n",
    "                    w_dt = doc_vector.get(term, 0)\n",
    "                    qtfi = query_weights.get(term, 0)\n",
    "                    \n",
    "                    # Get statistics\n",
    "                    n_i = self.term_doc_freq.get(term, 0)\n",
    "                    r_i_term = r_i.get(term, 0)\n",
    "                    \n",
    "                    # Compute the probability ratio with smoothing\n",
    "                    # Numerator: (r_i + 0.5) / (R - r_i + 0.5)\n",
    "                    num_numerator = r_i_term + 0.5\n",
    "                    num_denominator = R - r_i_term + 0.5\n",
    "                    \n",
    "                    # Denominator: (n_i - r_i + 0.5) / (N - n_i - R + r_i + 0.5)\n",
    "                    den_numerator = n_i - r_i_term + 0.5\n",
    "                    den_denominator = self.N - n_i - R + r_i_term + 0.5\n",
    "                    \n",
    "                    # Avoid division by zero and negative values\n",
    "                    for value in [num_denominator, den_numerator, den_denominator]:\n",
    "                        if value <= 0:\n",
    "                            value = 0.5\n",
    "                    \n",
    "                    probability_ratio = (num_numerator / num_denominator) / (den_numerator / den_denominator)\n",
    "                    \n",
    "                    # Take logarithm\n",
    "                    log_ratio = math.log(probability_ratio)\n",
    "                    \n",
    "                    # Extended BIR formula\n",
    "                    score += w_dt * qtfi * log_ratio\n",
    "            \n",
    "            rsv_scores[doc] = score\n",
    "        \n",
    "        return rsv_scores\n",
    "    \n",
    "    def rank_documents(self, query_id, query_terms):\n",
    "        \"\"\"Rank documents by RSV score\"\"\"\n",
    "        rsv_scores = self.compute_rsv(query_id, query_terms)\n",
    "        ranked_docs = sorted(rsv_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked_docs\n",
    "    \n",
    "    def display_relevance_analysis(self, query_id, query_terms):\n",
    "        \"\"\"Display relevance analysis for interpretation\"\"\"\n",
    "        if query_id not in self.relevance_data:\n",
    "            print(f\"No relevance data for {query_id}\")\n",
    "            return\n",
    "        \n",
    "        rel_docs = self.relevance_data[query_id]['relevant']\n",
    "        R = len(rel_docs)\n",
    "        \n",
    "        print(f\"\\nRELEVANCE ANALYSIS for {query_id}:\")\n",
    "        print(f\"Relevant documents: {rel_docs} (R={R})\")\n",
    "        print(f\"{'Term':<10} {'r_i':<6} {'n_i':<6} {'R-r_i':<8} {'n_i-r_i':<10} {'log_ratio':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in self.term_doc_matrix:\n",
    "                n_i = self.term_doc_freq.get(term, 0)\n",
    "                r_i = sum(1 for doc in rel_docs if doc in self.term_doc_matrix[term])\n",
    "                \n",
    "                # Compute the probability ratio\n",
    "                num_numerator = r_i + 0.5\n",
    "                num_denominator = R - r_i + 0.5\n",
    "                den_numerator = n_i - r_i + 0.5\n",
    "                den_denominator = self.N - n_i - R + r_i + 0.5\n",
    "                \n",
    "                probability_ratio = (num_numerator / num_denominator) / (den_numerator / den_denominator)\n",
    "                log_ratio = math.log(probability_ratio)\n",
    "                \n",
    "                print(f\"{term:<10} {r_i:<6} {n_i:<6} {R-r_i:<8} {n_i-r_i:<10} {log_ratio:<12.4f}\")\n",
    "        \n",
    "        # Show how relevance feedback affects scoring\n",
    "        print(f\"\\nRELEVANCE FEEDBACK IMPACT:\")\n",
    "        for term in query_terms:\n",
    "            if term in self.term_doc_matrix:\n",
    "                n_i = self.term_doc_freq.get(term, 0)\n",
    "                r_i = sum(1 for doc in rel_docs if doc in self.term_doc_matrix[term])\n",
    "                \n",
    "                # Without learning score component\n",
    "                without_learning = math.log((self.N - n_i + 0.5) / (n_i + 0.5))\n",
    "                \n",
    "                # With learning score component\n",
    "                with_learning = math.log(((r_i + 0.5)/(R - r_i + 0.5)) / ((n_i - r_i + 0.5)/(self.N - n_i - R + r_i + 0.5)))\n",
    "                \n",
    "                impact = with_learning - without_learning\n",
    "                direction = \"↑\" if impact > 0 else \"↓\" if impact < 0 else \"→\"\n",
    "                \n",
    "                print(f\"{term}: {without_learning:.4f} → {with_learning:.4f} ({direction} {impact:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f766c6",
   "metadata": {},
   "source": [
    "## Implementation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9875f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_relevance_data():\n",
    "    \"\"\"Prepare relevance data for learning\"\"\"\n",
    "    return {\n",
    "        'q1': {'relevant': {'D2', 'D4'}, 'non_relevant': {'D1', 'D3', 'D5', 'D6'}},\n",
    "        'q2': {'relevant': {'D2', 'D4'}, 'non_relevant': {'D1', 'D3', 'D5', 'D6'}},\n",
    "        'q3': {'relevant': {'D4', 'D1'}, 'non_relevant': {'D2', 'D3', 'D5', 'D6'}},\n",
    "        'q4': {'relevant': {'D2', 'D1'}, 'non_relevant': {'D3', 'D4', 'D5', 'D6'}},\n",
    "        'q5': {'relevant': {'D3', 'D6'}, 'non_relevant': {'D1', 'D2', 'D4', 'D5'}}\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Prepare data\n",
    "    relevance_data = prepare_relevance_data()\n",
    "    \n",
    "    # Initialize both models\n",
    "    print(\"=== EXTENDED BIR MODEL - CORRECTED FORMULAS ===\")\n",
    "    print(\"With vs Without Learning Data\\n\")\n",
    "    \n",
    "    bir_without_learning = ExtendedBIRWithoutLearning(\"results/inverted_index_weighted.txt\")\n",
    "    bir_with_learning = ExtendedBIRWithLearning(\"results/inverted_index_weighted.txt\", relevance_data)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = {\n",
    "        'q1': ['10%', '175'],\n",
    "        'q2': ['12%', '175'], \n",
    "        'q3': ['12%', 'D6'],\n",
    "        'q4': ['10%', 'D6'],\n",
    "        'q5': ['D6', '1']\n",
    "    }\n",
    "    \n",
    "    for query_id, query_terms in test_queries.items():\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"QUERY: {query_id} - Terms: {query_terms}\")\n",
    "        print(f\"Relevant docs: {relevance_data[query_id]['relevant']}\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        # Get rankings from both models\n",
    "        ranked_without = bir_without_learning.rank_documents(query_terms)\n",
    "        ranked_with = bir_with_learning.rank_documents(query_id, query_terms)\n",
    "        \n",
    "        # Display detailed analysis\n",
    "        bir_without_learning.display_scoring_breakdown(query_terms)\n",
    "        bir_with_learning.display_relevance_analysis(query_id, query_terms)\n",
    "        \n",
    "        # Display comparison\n",
    "        print(f\"\\nRANKING COMPARISON:\")\n",
    "        print(f\"{'Rank':<6} {'Without Learning':<20} {'RSV':<12} {'With Learning':<20} {'RSV':<12} {'Relevance':<15}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for i in range(min(len(ranked_without), 6)):\n",
    "            doc_without, score_without = ranked_without[i]\n",
    "            doc_with, score_with = ranked_with[i] if i < len(ranked_with) else (\"-\", 0)\n",
    "            \n",
    "            rel_without = \"RELEVANT\" if doc_without in relevance_data[query_id]['relevant'] else \"non-rel\"\n",
    "            rel_with = \"RELEVANT\" if doc_with in relevance_data[query_id]['relevant'] else \"non-rel\"\n",
    "            \n",
    "            print(f\"{i+1:<6} {doc_without:<20} {score_without:<12.4f} {doc_with:<20} {score_with:<12.4f} {rel_without}/{rel_with}\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(f\"\\nPERFORMANCE SUMMARY for {query_id}:\")\n",
    "        calculate_performance_metrics(query_id, ranked_without, ranked_with, relevance_data[query_id]['relevant'])\n",
    "\n",
    "def calculate_performance_metrics(query_id, ranked_without, ranked_with, relevant_docs):\n",
    "    \"\"\"Calculate and display performance metrics\"\"\"\n",
    "    \n",
    "    def precision_at_k(ranked_list, relevant_docs, k=3):\n",
    "        return sum(1 for doc, _ in ranked_list[:k] if doc in relevant_docs) / k\n",
    "    \n",
    "    def average_precision(ranked_list, relevant_docs):\n",
    "        score = 0.0\n",
    "        num_relevant = 0\n",
    "        for i, (doc, _) in enumerate(ranked_list):\n",
    "            if doc in relevant_docs:\n",
    "                num_relevant += 1\n",
    "                score += num_relevant / (i + 1)\n",
    "        return score / len(relevant_docs) if relevant_docs else 0\n",
    "    \n",
    "    p3_without = precision_at_k(ranked_without, relevant_docs, 3)\n",
    "    p3_with = precision_at_k(ranked_with, relevant_docs, 3)\n",
    "    ap_without = average_precision(ranked_without, relevant_docs)\n",
    "    ap_with = average_precision(ranked_with, relevant_docs)\n",
    "    \n",
    "    print(f\"Precision@3:   {p3_without:.4f} (without) → {p3_with:.4f} (with) → {p3_with - p3_without:+.4f} change\")\n",
    "    print(f\"Avg Precision: {ap_without:.4f} (without) → {ap_with:.4f} (with) → {ap_with - ap_without:+.4f} change\")\n",
    "    \n",
    "    # Check if learning improved the ranking\n",
    "    rel_pos_without = [i for i, (doc, _) in enumerate(ranked_without) if doc in relevant_docs]\n",
    "    rel_pos_with = [i for i, (doc, _) in enumerate(ranked_with) if doc in relevant_docs]\n",
    "    \n",
    "    if rel_pos_with and rel_pos_without:\n",
    "        print(f\"Best relevant doc position: {min(rel_pos_without)+1} → {min(rel_pos_with)+1}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
